\makeatletter \@ifundefined{rootpath}{\input{../../setup/preamble.tex}}\makeatother
\worksheetstart{Concurrency Basics}{1}{April 24, 2013}{Someone}{../../}
This chapter presents the basic concepts related to concurrency. The goals of the chapter is to ensure common ground between the authors and the reader. Furthermore describing these basic constructs separately allows for a more concise description of related topics in \bsref{chap:threads_locks}, \bsref{chap:stm} and \bsref{chap:actor}, discussing the selected concurrency models. 
\label{chp:con_basics}

\bsref{sec:concurrency_parallelism} presents how the literature describes the concepts of concurrency and parallelism as well as giving a definition of the two concepts. \bsref{sec:processes_threads} presents the process thread model on which concurrency is build and \bsref{sec:synchronization} describes the concept of synchronization.
\section{Concurrency \& Parallelism}\label{sec:concurrency_parallelism}
Concurrency and parallelism are two highly connected yet different concepts. While both deal with running multiple computations simultaneously, their reasons for doing so are different. Both concepts have varying definitions throughout the literature. As such we will present our definition of the concepts in order to clarify their meaning within this report. The definitions are based on relevant descriptions of concurrency and parallelism within the literature.

In \cite[p. 330]{papadopoulos1998coordination} concurrency is \textit{``cooperation among very large numbers of active entities that comprise a single application''} while \cite[p. 607]{cleaveland1996strategic}
says \textit{``Concurrency is concerned with the fundamental aspects of systems of multiple, simultaneous active computing agents that interact with one another''}. Both emphasise that concurrent applications consist of multiple computation entities, collaborating with one another.

Regarding concurrency \cite[p. 579]{sebestaProLang} states that concurrency can be either physical or logical. Physical concurrency is directly supported by a multicore processor, providing true simultaneous execution. Logical concurrency on the other hand, executes on a single core processor and concurrency is achieved by interleaving the execution of task.

Parallelism is closely related to concurrency. In \cite{introPar} the author states that in using parallelism \textit{``A problem is broken into discrete parts that can be solved concurrently''}, indicating that parallelism builds upon concurrency. A prerequisite for parallelism is that the problem at hand can be divided into the described discrete parts. Such problems are referred to as embarrassingly parallel tasks\cite{sutter2005software}. Here operations can be applied independently to each data point.

In \cite[p. 24]{sevenModels} the author states that concurrency deals with handling multiple tasks simultaneously, while parallelism is dividing a single task up into multiple parts and computing these parts simultaneously. The difference lies in the task that is to be performed. Performing multiple tasks simultaneously is considered concurrency, while splitting up a task in smaller parts and performing these parts simultaneously is considered parallelism.

Based on the descriptions of concurrency and parallelism presented here we give the definition of concurrency shown in \bsref{def:concurrency}, followed by the definition of parallelism presented in \bsref{def:parallelism}. The definition of parallelism is based on that of concurrency, reflecting the strong connection between the concepts. The description of parallelism found in \cite{introPar} supports such a connection.
\begin{defn}\label{def:concurrency}
\emph{Concurrency is executing multiple tasks simultaneously}
\end{defn}

\begin{defn}\label{def:parallelism}
\emph{Parallelism is dividing a single task into multiple independent parts, which are then executed concurrently}
\end{defn}
Distinguishing between concurrency and parallelism is based on what is considered the task to solve. If a single task is divided into multiple smaller parts, it is considered parallelism. On the other hand solving multiple tasks simultaneously is considered concurrency.

As an example, handling multiple requests to a webservice simultaneously, is considered concurrency. On the other hand calculating the sum of a list of integers, by dividing the list into multiple smaller lists and calculating their sums simultaneously, is considered parallelism.

\section{Processes \& Threads}\label{sec:processes_threads}
An \ac{OS} has to run multiple programs simultaneously. In order to do so, the \ac{OS} use an abstraction of a running program, called a process\cite[p. 81]{tanenbaum2008modern}\cite[p. 16]{bryant2011computer}. Multiple processes can be executed concurrently and the \ac{OS} provides processes with the illusion of having exclusive access to the hardware\cite[p. 16]{bryant2011computer}. 

A single \ac{CPU} core can only execute a single process at a time. On such a single core system, the \ac{OS} interleaves the instructions of processes producing the illusions, of processes being executed simultaneously\cite[p. 16]{bryant2011computer}. On more modern multi core systems, each core can execute a single process at a time. In such systems, interleaving is still performed if the number of processes is greater than the number of cores.

Interleaving of processes is performed using what is called a context switch\cite[p. 16]{bryant2011computer}. A context switch is the act of the \ac{OS} saving the currently running process's context and restoring the context of the process that is to run instead. The context is the state of the process, including information such as, its registers and variables\cite[p. 82]{tanenbaum2008modern}. As such, a context switch from process \bscode{A} to process \bscode{B} is essentially saving the state of \bscode{A} and restoring the state of \bscode{B}, allowing \bscode{B} to resume where it left off, at the expense of \bscode{A}.

Processes consists of two separate concepts: resources and execution \cite[p. 98]{tanenbaum2008modern}. The resources of a process include an address space in memory, the program code as well as information on open files etc. The execution of a process is handled by what is called a thread\cite[p. 98]{tanenbaum2008modern}. A thread is an abstraction for a part of a process, that executes sequentially. A thread has its own program counter, tracking where in the program code the thread currently resides, as well as a stack that tracks the threads execution history\cite[p. 99]{tanenbaum2008modern}. Threads are the part of a process, that is scheduled for execution upon the \ac{CPU}.

A process has at least one thread which handles its execution, but can have more. Having multiple threads facilitates concurrency within the process. Threads within the same process share the address space of the process and can communicate by, reading from and writing to the memory within that address space. Threads are the basic construct for concurrency within programs and concurrency models build upon threads in some form.

The process thread model is exemplified in \bsref{fig:classic_process_thread}. Here the processes \bscode{A} and \bscode{B} are shown. Process \bscode{A} has a single thread while process \bscode{B} has three. Process \bscode{B}'s threads share the address space assigned to process \bscode{B} and can communicate by reading from and writing to the memory within that address space. The threads of process \bscode{B} do not share memory with the thread of process \bscode{A}.

\begin{figure}[htbp]
\centering
 \includegraphics[width=0.8\textwidth]{\rootpath/worksheets/concurrency_basics/figures/process_thread} 
 \caption{The classic process thread model. Inspired by \cite[p. 99]{tanenbaum2008modern}.}
\label{fig:classic_process_thread}
\end{figure}


\subsection{Hardware \& Software Threads}
The threads discussed in \bsref{sec:processes_threads} are refereed to as hardware threads, as they are directly supported by the hardware and \ac{OS}. The hardware must support a set of instructions which the \ac{OS} uses to maintain the thread abstraction. Hardware threads can utilize multiple cores and take advantage of the \ac{OS}'s scheduling.

Another type of thread is what is called a software thread. Software threads are implemented purely in software, on top of the hardware and \ac{OS}. They have no direct connection to the underlying hardware and \ac{OS} and because of this can not take advantage of the scheduling provided by the \ac{OS} or use additional cores if available.

\section{Synchronization}\label{sec:synchronization}
Threads within the same process are executed concurrently. Synchronization is the act of using language or library constructs to restrict the concurrent execution, in order to remove invalid interleaving of threads\cite[p. 1989]{scott2011sync}.

Two main types of synchronization exists: atomicity and conditional synchronization. Atomicity ensures that, the execution of a set of instructions or statements, executed by some thread, is seen as single step, from the point of view of every other thread\cite[p. 1989]{scott2011sync}.

Conditional synchronization forces the executing thread to wait until some locally observable precondition is true\cite[p. 1989]{scott2011sync}. Conditional synchronization can, for example, be used for waiting on some parallel work to finish, before reading the result.

From the point of view of the programmer, providing synchronization differs greatly depending on the chosen concurrency model. For concurrency models based on message passing, synchronization is generally embedded in the message passing constructs. On the other hand, concurrency models based on shared memory generally require the programmer to use additional constructs for synchronization\cite[p. 1989]{scott2011sync}.

\subsection{Shared Memory Synchronization}
In the context of shared memory concurrency, two basic types of synchronization exists: busy-waiting and scheduler-based\cite[p. 1990]{scott2011sync}. Busy waiting synchronization uses \ac{CPU} time while scheduler-based does not. Scheduler-based synchronization does however deschedule the thread on which synchronization is required, requiring two context switches. One for descheduling the thread in order to supply synchronization and one for rescheduling it when it becomes runnable again.

Synchronization is supported by a number of hardware primitives, referred to as read-modify-write instructions\cite[p. 1990]{scott2011sync}. Read-modify-write instructions execute atomically, that is, no other instructions can be executed between parts of the operation. Test-and-set and compare-and-swap are example of read-modify-write instructions.  
\worksheetend
