\makeatletter \@ifundefined{rootpath}{\input{../../setup/preamble.tex}}\makeatother
\worksheetstart{Method}{1}{April 24, 2013}{Andreas}{../../}
\section{Research Approach}
The research will be divided into two distinct phases. The first, begin a preliminary analysis of related work with regards to concurrency models. The purpose of which is to gain an overview of existing concurrency models and as well as their current state of research. The second part will be a in-depth analysis of a number of concurrency models, selected based on the findings in the preliminary analysis. The analysis will describe the characteristics of the selected models along with their strengths and weaknesses. The analysis along with a performance test of the selected models will be the basis for the conclusion. 

\section{Definition of Performance Test Approach}
In order to investigate the performance of the selected models, a performance test will be conducted. The $k$-means clustering algorithm has been selected as the basis for the comparison. $k$-means is a good basis for performance analysis of concurrency models as it is computationally demanding and easily partitioned into concurrent tasks\cite[p. 128]{epstein2011towards}.

$k$-means aims at clustering $n$ data points, into $k \leq n$ clusters\cite[p. 451]{dataminingconceptsandtechniques}\cite[p. 128]{epstein2011towards}. Each cluster $k$ is initially assigned a random centroid representing the mean of the cluster. For each of the data points, a distance to the centroid of each of the $k$ clusters is calculated. The data point is then assigned to the cluster to which it is closest. After every data points has been assigned, new centroids are calculated, based on the data points assigned to the clusters. The process is then repeated, using the newly calculated clusters centroids as the basis for the distance calculations. The algorithm terminates when the cluster centroids stop changing or when a set number of iterations has been completed\cite[p. 128]{epstein2011towards}. The algorithm is exemplified in \bsref{fig:kmeans}. Centroids are represented by triangles and data points by circles.

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.3]{\rootpath/worksheets/method/figures/kmeans}
\caption{On the left data points are assigned to the nearest centroid (cluster) based on the calculated distances. On the right, a new centroid is computed for each cluster.}\label{fig:kmeans}
\end{figure}

Two tests, each with their own purpose, will be conducted. Both tests will be based on a data set of vectors, where each vector consists of 100 integer data points. In the context of performance analysis, only the effort taken to cluster the data is of interest, as opposed to the resulting clustering. As a result, randomly generated data is sufficient.

The first test will be based on a fixed size data set, while the number of concurrent tasks will be scaled up. The goal is to determine how the selected concurrency models, scale as more concurrent tasks are added. The test will be based on a data set of 1,000,000 vectors. Five clusters will be created, using a maximum of eight concurrent tasks. The algorithm will run for 10 iterations.

The second second test will be based on a fixed number of concurrent tasks, while the size of the dataset will be scaled up. The goal of this test is to determine how the selected concurrency model, scale as the size of the dataset increases. The test will be conducted using four concurrent tasks, while scaling the data set from 100,000 vectors to 5,000,000 vectors. As with the previous test, five clusters will be created and the algorithm will run for 10 iterations.

The parameters of the two tests are depicted in \bsref{tab:test_description}.

\begin{center}
\begin{table}[h]
\begin{tabular}{c|cccc}
       & \# Vectors        & \# Concurrent tasks 	& \# Clusters & \# Iterations \\ \hline
Test 1 & 1,000,000         & 1-8        			& 5           & 10            \\
Test 2 & 100,000-5,000,000 & 4          			& 5           & 10           
\end{tabular}
\end{table}
\captionof{table}{Description of tests} \label{tab:test_description} 
\end{center}

\kasper{Ved ikke om vi skal forklare map reduce?}
The implementations will follow a template similar to that of Map-Reduce\cite{dean2008mapreduce}, know from functional programming, as well as the Google and Hadoop MapReduce frameworks. MapReduce employs a set of mappers to apply a function to each data point. This function outputs a number of results, that are then aggregated to the final result using a reducer. This is analogous to the \bscode{map} and \bscode{reduce} functions, know from functional programming. For the $k$-means performance test, a set of mappers will be assigned a set of vectors, for which it will calculate the distance to each of the $k$ cluster centroids and assign it to the nearest cluster. A reducer will then aggregate the results from the mappers and calculate the new cluster centroids. The process will continue until the stated 10 iterations are achieved.

In order to produce comparable results across multiple implementations, the languages employed will be limited to languages running on the \ac{JVM}.\toby[inline]{1. Lav en tabel over test metoder 2. inds√¶t hardware specs for setup}

\subsection{Distance Measure}
A large part of the computations involved in the $k$-means algorithm, lies in calculating the distance between each of the data points and the cluster centroids. Different ways of calculating this distance exists. Simple, yet efficient, calculations such as Manhattan and Euclidean\cite[p. 41]{amatriain2011data} distances are fast to compute. Where the more complex similarity measures as Pearson Correlation Coefficient and Cosine similarity\cite[p. 42]{amatriain2011data}\cite{breese1998empirical} are slower to compute but provide a more accurate result.

The \ac{PCC} has been chosen for the performance test, as it represents one of the most complex computations. Having a complex distance calculation will increase the workload which will be run concurrently, increasing the focus on the concurrency aspect of the performance test. We are aware that choosing a complex calculation pushes the results towards favoring concurrent execution over sequential execution, but the focus on concurrency has been a higher concern. The \ac{PCC} is defined as:

\begin{equation}\label{pearsonverbose}
\frac{cov(a,i)}{\sigma_a \times \sigma_i} = \frac{\sum_j(v_{a,j}-\bar{v}_a)(v_{i,j}-\bar{v}_i)}{\sqrt{{\sum_j}(v_{a,j}-\bar{v}_a)^2 \sum_j(v_{i,j}-\bar{v}_i)^2}}
\end{equation}

where $cov(a,i)$ is the covariance between the two vectors $a$ and $i$ and $\sigma_a \times \sigma_i$ is the product of the vectors standard deviations.


\section{Documentation}


\worksheetend