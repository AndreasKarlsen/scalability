\makeatletter \@ifundefined{rootpath}{\input{../../setup/preamble.tex}}\makeatother
\worksheetstart{Threads \& Locks}{1}{April 24, 2013}{Andreas}{../../}
This chapter presents the \ac{TL} concurrency model. Describing its key concepts and building blocks. \kasper{Reference sections when done}
\kasper[inline]{Section on why Java was selected for TL implementation and discussion}
\kasper[inline]{Section on more advanced locking constructs, such as java synchronized}
\kasper[inline]{Maybe a chapter earlier on basic thread/process relationship}
\label{chap:threads_locks}
\section{Key Concepts}
This section will present the key concept of the \ac{TL} concurrency model. 
\subsection{Threads}\label{subsec:threads}
Threads are the basic building block for concurrency in many concurrency models and in particular the \ac{TL} model. A thread is a abstraction for a part of a application that executes sequentially. When a application is started the \ac{OS} creates a single thread along with a memory space for the application. These together handle the execution of the application. Concurrency can be achieved by, within a single application, running multiple threads. Threads running within the same application have access to shared memory in order to facilitate communication. They are the basic construct for concurrency and other concurrency models build upon threads in some form.

Two groups of threads exist: hardware threads and user library threads. Hardware threads are directly supported by the underlying hardware and \ac{OS}. The hardware must support a set of instructions which the \ac{OS} uses to maintain the thread abstraction. Hardware threads can utilize multiple cores as support for them is build directly into the underlying hardware and \ac{OS}. The \ac{OS} handles scheduling of the threads.

User library threads are implemented purely in software on top of the hardware and \ac{OS}. They have no direct connection to the underlying hardware and \ac{OS} and because of this can not take advantage of their scheduling or use additional cores if available. Because of these limitations the content of this report is restricted to native threads.

\subsection{Shared Memory}\label{sub:sec:shared_memory}
Threads have access to shared memory, in order to facilitate communication between them\cite[p. 93]{tanenbaum2008modern}. Threads can communicate by reading from and writing to the shared memory. As described in \bsref{subsec:threads} once a program is started the \ac{OS} creates a single thread and along with that a memory space for the program. If more threads are created they have their own stack but otherwise share the same memory space.

Accessing shared data facilitates fast communication between threads, but also presents a number of issues such as race conditions. If threads do not read data that can be modified from other threads, these issues are not present. So multiple threads can run without issue as long as they do not read any shared data or even as long as they do not modify any shared data.

\kasper[inline]{Look in seven models}

\subsection{Locks and Mutual Exclusion}\label{sec:locks_me}
Mutual exclusion is the property of shared memory concurrency which ensures that only a single concurrent task can access a given critical region at a time\cite[p. 117]{tanenbaum2008modern}\cite[p. 962]{bryant2011computer}, a critical region being a concurrent task accessing some memory that is shared with other concurrent tasks\cite[p. 117]{tanenbaum2008modern}\cite[p. 961]{bryant2011computer}. Having only a single concurrent task execute within all critical regions at a time, ensures that the program will behave as if it was run sequentially, even though this might not be the case. Access to critical regions are often restricted by the use of different forms of locks\cite[p. 58]{sutter2005software}. 

Locks are a general class of constructs used for providing mutual exclusion. Locks come in a variety of different fashions such a Mutex, Monitor and Semaphore. Each of these operate in their own way, but the overall goal remains the same. Locks provide mutual exclusion by allowing only a single concurrent task to enter its critical region at a time. When a concurrent task \bscode{t1} attempts to access a lock that is all ready held by another task \bscode{t2},\bscode{t1} blocks, effectively waiting for the \bscode{t2} to exit its critical region and release the lock before proceeding. Locks prevent race conditions but introduce other issues such as: deadlocks, priority inversion and starvation (which will be presented in \bsref{sec:tl_ci}) and leads to concurrent tasks spending time blocked instead of executing parts of the program. Furthermore, the programmer is left alone to place the locks in the correct place and the correct order, which in itself is hard. If he succeeds of using locks as synchronization mechanism, a major caveat is that they are not composable\cite[p. 58]{sutter2005software}. That is, you cannot compose two correct lock-based pieces of code and know the result is still correct. This significantly limits the reuseability of software components, and therefore lowers the productivity of the programmer.
\kasper{Maybe move parts of above paragraph to characteristics chapter discussion chapter later}

Consider the example presented in \bsref{lst:mutualexclusion} using Java threads and Locks. Here we see a small Java\lone{Hvorfor Java?} program that starts two threads and waits for each of them to finish, printing out the result of their computations before exiting. As it can be seen in lines 11-15 and 18-19 the two threads have had their critical regions locked, limiting the access to one thread at a time. On line 12 \bscode{t1} checks if the static variable \bscode{number} is equal to ten (which is its initial value) while it on line 13 sets the \bscode{result} variable to \bscode{number} times three if it the condition was true. As can be seen on line 19 the second thread simply sets the value of \bscode{number} to 20. It is clear that, if \bscode{t1} was executed before \bscode{t2} the result would look as follows:
\begin{verbatim}
Result is: 30
\end{verbatim}
If however \bscode{t2} was executed before \bscode{t1} the result would be:
\begin{verbatim}
Result is: 0
\end{verbatim}
If either \bscode{t1} or \bscode{t2} attempts to acquire the lock while the other already holds it they would block ensuring mutual exclusion.

%\bsref{lst:mutualexclusion} depicts a modified version of the example presented in \bsref{lst:racecondition}. The two threads have had their critical regions locked, limiting the access to one thread at a time. Because of this, the case where \bscode{t1} runs just until its about to compute the result and  \bscode{t2} taking over will no longer occur. \bscode{t2} will now not be able to change the value of number as \bscode{t2} still holds the lock on the critical region, even though it is paused. As a result the output result is: 60, can no longer occur. The output does however still depend on the order in which access to the critical regions is acquired.
\begin{lstlisting}[float,label=lst:mutualexclusion,
  caption={Mutual exclusion in Java using a lock},
  language=Java,  
  showspaces=false,
  showtabs=false,
  breaklines=true,
  showstringspaces=false,
  breakatwhitespace=true,
  commentstyle=\color{greencomments},
  keywordstyle=\color{bluekeywords},
  stringstyle=\color{redstrings}]  % Start your code-block

	import java.util.concurrent.locks.Lock;
	import java.util.concurrent.locks.ReentrantLock;

	public class MutualExclusion {
    private static int number = 10;
    private static int result = 0;
    private final static Lock lock = new ReentrantLock();

    public static void main(String[] args) throws InterruptedException {
        Thread t1 = new Thread(() -> {
            lock.lock();
            if (number == 10){
                result = number * 3;
            }
            lock.unlock();
        });
        Thread t2 = new Thread(() -> {
            lock.lock();
            number = 20;
            lock.unlock();
        });
        t1.start(); t2.start();
        t1.join(); t2.join();
        System.out.println("Result is: " + result);
    }
	}
\end{lstlisting}

\subsection{Additional features}
%Executor service
%Futres
%Still based on threads

\section{Concurrency Issues}\label{sec:tl_ci}
This section presents an overview of known issues related to \ac{TL} concurrency. Many of the issues arise as a result of access to shared memory and synchronization of this access. Some of the issues are also present in other concurrency models.
%Some of the issues are related mainly to shared memory concurrency while others apply to a broader spectrum, including asynchronous message passing.

\subsection{Race Conditions}\label{subsec:race_coditions}\kasper{Time line of how threads execute}
One issue that can arise in concurrent applications, is that the result of a computation, consisting of two or more concurrent tasks that share data, depends on how the concurrent tasks are scheduled. That is the behaviour of the application depends on the timing of concurrent tasks. This issue is called a \emph{race condition}
\cite[p. 983]{bryant2011computer}\cite[p. 115]{tanenbaum2008modern}\cite[p. 44]{sevenModels}. 

Consider the example presented in \bsref{lst:racecondition}, which is similar to that of \bsref{lst:mutualexclusion}, except the lock ensuring mutual exclusion on lines 11-15 and 18-20 has been removed. As a result a race condition is now present. Imagine that \bscode{t1} runs just until its about to compute the result in line 8, at which point it is paused by the operating system and \bscode{t2} get to run instead. \bscode{t2} now executes the lambda shown on line 11 and sets the value of \bscode{number} to 20, after which it exits causing \bscode{t1} to be resumed. \bscode{t1} then runs line 8, computing the result based on the new \bscode{number} value of 20, causing the result to be:
\begin{verbatim}
Result is: 60
\end{verbatim}
instead of 30 as expected. The previous outputs of 0 and 30 are still possible, but the output now depends on the timing of the concurrent tasks. Race conditions can be very hard to find and remove as they can result in incorrect values instead of errors and because they do not necessarily occur on every program execution. Race conditions can be avoided by providing mutual exclusion as discussed in \bsref{sec:locks_me},
\begin{lstlisting}[label=lst:racecondition,
  caption={Race condition example},
  language=Java,  
  showspaces=false,
  showtabs=false,
  breaklines=true,
  showstringspaces=false,
  breakatwhitespace=true,
  commentstyle=\color{greencomments},
  keywordstyle=\color{bluekeywords},
  stringstyle=\color{redstrings}]  % Start your code-block

	public class RaceCondition {
    private static int number = 10;
    private static int result = 0;

    public static void main(String[] args) throws InterruptedException {
        Thread t1 = new Thread(() -> {
            if (number == 10){
                result = number * 3;
            }
        });
        Thread t2 = new Thread(() -> number = 20);
        t1.start(); t2.start();
        t1.join(); t2.join();
        System.out.println("Result is: " + result);
    }
	}
\end{lstlisting}
 
\subsection{Deadlocks}
A deadlock occurs when all concurrent tasks in a set are waiting on some event, that can only be caused by another task in the set\cite[p. 435]{tanenbaum2008modern}. Because all the concurrent tasks are blocked and waiting on one of the other tasks, none of them will ever continue and they will block indefinitely.

As an example consider \bsref{fig:deadlockexample}. Here the three threads T1, T2 and T3 are depicted. The three threads are all waiting on a resource, that has been acquired by one of the other threads, as depicted by the arrows. Each thread is waiting on a resource held by one of the other threads and is per our definition of a deadlock, the event being the release of the resource, deadlocked.
\begin{figure}[htbp]
\centering
 \includegraphics[width=0.5\textwidth]{\rootpath/worksheets/threads_and_locks/figures/deadlock} 
 \caption{Deadlock example}
\label{fig:deadlockexample}
\end{figure}

Deadlocks occur for different reasons in different concurrency models. As described \bsref{fig:deadlockexample} depicts a example of a deadlock using \ac{TL}. Here the deadlock is caused by threads acquiring a lock on a resource they need, where after they attempt to acquire a lock on a resource that is held by another thread, that is directly or indirectly waiting on \bscode{R}. This type of deadlock is called a resource deadlock\cite[p. 435]{tanenbaum2008modern}. 

%Another type of deadlock that can occur in, for example, the Actor concurrency model is called a \emph{communication deadlock}\cite[p. 124]{tanenbaum2008modern}. Instead of deadlocks being caused by locks, a deadlock is here cause by actors waiting on messages. In the context of \bsref{fig:deadlockexample} this would mean that T1, T2 and T3 where actors and the arrows represented an actor waiting on a message from a given actor, so that T1 is waiting on a message from T2. A communication deadlock can occur both over network as well as on a single machine.
%threads and lock = resource aqusision
%async message passing = wating on messages as with resources
\kasper[inline]{Deadlock detection}
\kasper[inline]{Deadlock recovery}

\subsection{Livelocks} A livelock is similar to a deadlock in that no progress is being made. While a deadlocked concurrent task is blocked, a livelocked concurrent task is till executing. The execution does however not result in any progress. A livelock can occur when multiple concurrent tasks attempts to recover from a error such as a deadlock\cite[p. 457]{tanenbaum2008modern}. Consider two concurrent tasks \bscode{A} and \bscode{B}. \bscode{A} and \bscode{B} both attempt to write to some common storage where after they check if the data was written correctly. If that is not the case they attempt the write again. This could lead to another failed attempt and another retry, which could go on indefinitely. \bscode{A} and \bscode{B} are then said to be livelocked.

In this scenario the livelock could be avoided by having each task wait a random period of time $t < m$, where m is the current maximum wait time, before retrying. Having $m$ grow for each time a task has to retry its operation minimizes the risk of the other tasks interfering with the write as it is more likely that the retry will occur at different times. It is however also of interest to minimize wait time as no progress is made during this period.
\kasper[inline]{Source!}

\subsection{Priority Inversion}
Priority inversion is the problem of a high priority concurrent tasks blocking, while waiting for a low priority concurrent task to finish\cite[p. 456]{tanenbaum2008modern}. As an example consider the two concurrent tasks \bscode{H} and \bscode{L} shown in \bsref{fig:priority_inversion}. \bscode{H} has a high priority while \bscode{L} has a low priority. \bscode{L} starts running, enters its critical region and acquires a lock on some shared resource \bscode{R}. The high priority task \bscode{H} is then started. It attempts to acquire a lock on the resource \bscode{R} in order to enter its critical region. The lock on \bscode{R} is however still held by \bscode{L} and the high priority task \bscode{H} must now wait for the low priority task \bscode{L} to finish.

\begin{figure}[htbp]
\centering
 \includegraphics[width=0.95\textwidth]{\rootpath/worksheets/threads_and_locks/figures/pi_graf} 
 \caption{Priority inversion example}
\label{fig:priority_inversion}
\end{figure}
If a concurrent task \bscode{M} with medium priority was introduced, scenario could instead looks as depicted in \bsref{fig:priority_inversion_m}. The scenario is similar to the previous but here \bscode{M} starts up after \bscode{L} has acquired the lock on \bscode{R}. Because \bscode{M} is a medium priority task it is scheduled before \bscode{L}. As a result \bscode{H} has to wait on both a low and medium priority task to finish before acquiring the lock on \bscode{R}.

\begin{figure}[htbp]
\centering
 \includegraphics[width=0.95\textwidth]{\rootpath/worksheets/threads_and_locks/figures/pi_graf2} 
 \caption{Priority inversion with medium priority task}
\label{fig:priority_inversion_m}
\end{figure}

\subsection{Starvation}
A concurrent task is said to be starved if, it is denied access to a resource indefinitely\cite[p. 459]{tanenbaum2008modern}. The task can be denied based on many different criteria. For example, priority or the size of the job that it is to perform.

As an example, imagine a scheduler that gives concurrent tasks access to some resource R based on the priority of the task. Tasks with a high priority gets access to the resource before tasks with a low priority. Now imagine a low priority concurrent task \bscode{A} attempts to acquire a lock on some resource \bscode{R}. The lock on \bscode{R} is however currently held by some other task and \bscode{A} is blocked waiting for access to \bscode{R}. While \bscode{A} is blocked, a high priority task \bscode{B} also attempts to acquire the lock on \bscode{R} and is blocked as well. When the lock on \bscode{R} is released it is given to the high priority task \bscode{B} even though \bscode{A} blocked first and \bscode{A} remains blocked. If this scenario keeps repeating and a high priority task keeps getting the lock over \bscode{A}, \bscode{A} is said to be starved.

%The same scenario can without the use of locks for synchronization. Imagine a printing service in a asynchronous message passing system. The printing service accepts, schedules and handles print jobs on behalves of a number of clients. The printing services givers a higher priority to jobs created by some clients than jobs created by others. If a client submitted the low priority job \bscode{A} to the printing service, but the queue of the printing service already contained a number of high priority jobs, \bscode{A} would have to wait until the high priority jobs where finished. If the printing services keeps receiving new high priority jobs at a pace such that the queue allways contains atleast a single high priority job, then \bscode{A} will never be executed and will be starved. The similar scenario could occur if the printing service gave priority to the smallest job. A very large job might starve as smaller jobs keep getting prioritized

Starvation can be avoided using a \ac{FIFO} scheduling strategy\cite[p. 459]{tanenbaum2008modern}.

\section{Discussion}
This section contains a discussion of a number of issues related to \ac{TL} concurrency. Why \ac{TL} concurrency is considered to be hard is discussed in \bsref{subsec:tl_lock_hard} which is followed by a discussion of the composability of the \ac{TL} concurrency model in \bsref{subsec:tl_lock_hard}.
\subsection{Why locks are hard}\label{subsec:tl_lock_hard}
Concurrency is generally considered to be hard, the same goes for using locks to provide mutual exclusion\cite[p. 56]{sutter2005software}. Several reason for this exists. One of them is that in order for locks to work, programmers have to strictly follow a set of conventions. If, for example, the programmer takes locks in the wrong order, it can potentially lead to deadlocks\cite[p. 58]{sutter2005software}. Furthermore the relationship between a lock and the shared data that is is protecting is implicit, that is, it is enforced by the programmer. If the programmer misses or leaks a reference to the shared data, mutual exclusion is no longer ensured and race conditions can occur. This is worsened by locking being a global property that requires local handling. That is, data might be shared between many parts of a system and each part have to locally reason about locking access to the data.

Locks also has the unfortunate problem of not mixing well some aspects of \ac{OOP}. Encapsulating data with objects that only allow modification to the data via the objects interface, is well know concept from \ac{OOP}. Ensuring that multiple threads accessing a object does not corrupt its internal structure, can be done by encapsulating locking, along with the data, inside the object. As an example consider the \bscode{Account} class presented in \bsref{lst:account_example}. The \bscode{Account} class uses a internal lock, which is defined on line 4, to ensure that only a single thread, can modify the balance of the account at a time. This solution does however have a number of issues. Firstly, if a programmer where the extend the \bscode{Account} class and provides a overridden version of the credit or debit methods, she would have to ensure that mutual exclusion on the balance was maintained. In this simple example such as task could be achieved without to much trouble, assuming that the programmer extending the account class knows about the lock. If however it had been a more complex class, using more sophisticated locking, providing a overridden method might not be a simple task. In any case it is a task with a high potential for programmer error, especially if the programmer does not have access to the source code of the class that is to be extended.

\begin{lstlisting}[float,label=lst:account_example,
  caption={Encapsulated locking},
  language=Java,  
  showspaces=false,
  showtabs=false,
  breaklines=true,
  showstringspaces=false,
  breakatwhitespace=true,
  commentstyle=\color{greencomments},
  keywordstyle=\color{bluekeywords},
  stringstyle=\color{redstrings}]  % Start your code-block

	public class Account {

    private int balance;
    protected final Lock lock = new ReentrantLock();

    public Account(int balance){
       this.balance = balance;
    }

    public void credit(int amount){
        lock.lock();
        balance += amount;
        lock.unlock();
    }

    public void debit(int amount){
        lock.lock();
        balance -= amount;
        lock.unlock();
    }
	}
\end{lstlisting}

Secondly consider the example of transferring funds from one account to another. \bsref{lst:account_example_transfer} shows one way in which this could be achieved. The two accounts \bscode{a1} and \bscode{a2} are created with balances of 500 and 100 respectively. 200 is debited from \bscode{a1} followed by crediting \bscode{a2} with 200 completing the transfer. The problem arises because the encapsulated locking only protects each call to debit and credit but not the state in between them\cite[p. 59]{sutter2005software}. A thread can read the inconsistent state of the accounts in between the debiting of \bscode{a1} and crediting of \bscode{a2}. Additional locking has to be provided  if this is to be avoided.

\begin{lstlisting}[float,label=lst:account_example_transfer,
  caption={Funds transfer between two accounts},
  language=Java,  
  showspaces=false,
  showtabs=false,
  breaklines=true,
  showstringspaces=false,
  breakatwhitespace=true,
  commentstyle=\color{greencomments},
  keywordstyle=\color{bluekeywords},
  stringstyle=\color{redstrings}]  % Start your code-block

    Account a1 = new Account(500);
    Account a2 = new Account(100);
    a1.debit(200);
    a2.credit(200);
\end{lstlisting}

In \cite{lee2006problem} the author argue that introducing threads into a program makes the program nondeterministic, that is, the program can produce different results for each time its run. This is exactly the effect race conditions, described in \cite{subsec:race_coditions}, have on a program. The authors argue that it becomes the job of the programmer to prune away this nondeterminism in order to produce a deterministic program. Using the \ac{TL} concurrency model this is done using different forms of lock. However such pruning is not easy task and suing the \ac{TL} model introduces a number of other issues such as deadlocks and starvation.

%Some locking constructs, such as Java's syncronized methods, provide object based locking.
%\subsection{\ac{TL} and OOP}

\subsection{Composability}\label{subsec:tl_composability}
As mentioned in \bsref{sec:locks_me}, lock based concurrent implementations are not composable. Creating new software by composing existing implementations, in the form of libraries, is a common practice in software development. If such libraries uses locks to correctly ensure mutual exclusion, it is not guaranteed that combining these libraries, results in a application free of concurrency related errors.

Deadlocks are the primary reason for locks not composing\cite[p. 58]{sutter2005software}. Event based frameworks call code that has be defied by programmers using the framework to signal that some event occurred. If such frameworks uses locks to ensure correct synchronization, calling code which the framework itself knows nothing about, while holding locks, can lead to deadlocks.

As a example consider the observer pattern\cite{gamma1994design} in which an object (the observable) is observed by a number of observers. Observers can register and unregister for notifications and the observable notifies any registered observers whenever some event occurs. 

\bsref{lst:observer} show the implementation of a observable that attempts to use a semaphore in order to ensure thread safety. The \bscode{ValueStore} class implements the \bscode{Observable} interface, overriding the two methods \bscode{register} and \bscode{unregister} defined on the interface. Besides these two methods the  \bscode{ValueStore} class defines the set value method which allows others to set the stored value as well as triggering notification of all registered observers. As mentioned the \bscode{ValueStore} class uses a semaphore to ensure mutual exclusion. The semaphore locks access to the registering and unregistering of observers as seen on lines 18-20 and 25-27 respectively. Furthermore the \bscode{ValueStore} class locks setting the internal value and notifying all registered observers as seen on lines 8-13.

\begin{lstlisting}[label=lst:observer,
  caption={Observer pattern with locks},
  language=Java,  
  showspaces=false,
  showtabs=false,
  breaklines=true,
  showstringspaces=false,
  breakatwhitespace=true,
  commentstyle=\color{greencomments},
  keywordstyle=\color{bluekeywords},
  stringstyle=\color{redstrings}]  % Start your code-block

	public class ValueStore implements Observable{

    private int value = 0;
    private List<Observer> observers = new ArrayList<>();
    private Semaphore sem = new Semaphore(1);

    public void setValue(int newValue) throws InterruptedException {
        sem.acquire();
        value = newValue;
        for (Observer o : observers) {
            o.notify(this,value);
        }
        sem.release();
    }

    @Override
    public void register(Observer observer) throws InterruptedException {
        sem.acquire();
        observers.add(observer);
        sem.release();
    }

    @Override
    public void unregister(Observer observer) throws InterruptedException {
        sem.acquire();
        observers.remove(observer);
        sem.release();
    }
	}
	
	public class ValueObserver implements Observer {

    @Override
    public void notify(Observable sender, int value) throws InterruptedException {
        sender.unregister(this);
    }
	}

	public class Main {

    public static void main(String [ ] args) throws InterruptedException {
        ValueStore observable = new ValueStore();
        observable.register(new ValueObserver());
        observable.setValue(5);
        System.out.println("Done");
    }
	}
\end{lstlisting}

The \bscode{ValueObserver} class implements the observer interface and overrides its notify method. The notify method simply unregisters from the observable supplying the notification.

The \bscode{Main} class and its \bscode{main} method defines the behaviour of the program. On line 42 a \bscode{ValueStore} is created followed by the registration of a \bscode{ValueObserver}, setting the value of the \bscode{ValueStore} to 5 and printing \bscode{"Done"} to the console. The printing will however never occur as the program deadlocks before it can happen. The deadlock occurs because the \bscode{ValueObserver} attempts to unregister from the \bscode{ValueStore} while the \bscode{ValueStore} hold the lock. As part of setting the value on line 44 the \bscode{ValueStore} acquires the lock sets the value and proceeds to notify all observes while still holding the lock, as seen on lines 8-13. Notifying a observer entail calling its \bscode{notify} method, which in the case of the \bscode{ValueObserver} means the \bscode{ValueObserver} unregistering from the observable. As the \bscode{unregister} method attempts to acquire the lock currently help by the \bscode{ValueStore} within the \bscode{setValue} method a deadlock is created resulting in the \bscode{"Done"} string never being printed.

In order to remove the deadlock one could move the notification of observers in the \bscode{setValue} method outside of the locked section. This would however mean that access to the list of observers would not be thread safe as nothing prevents the registration and unregistration of observers while the \bscode{setValue} method iterating over it. One way to solve this, would be to produce a shallow copy of the obeserver list within the locked section and iterate over the copy outside the locked section. This eliminates the call to outside defined code while holding locks. This idea is exemplified in the update version of the \bscode{setValue} method depicted in \bsref{lst:observer_updated}.

\begin{lstlisting}[float,label=lst:observer_updated,
  caption={Observer pattern with locks},
  language=Java,  
  showspaces=false,
  showtabs=false,
  breaklines=true,
  showstringspaces=false,
  breakatwhitespace=true,
  commentstyle=\color{greencomments},
  keywordstyle=\color{bluekeywords},
  stringstyle=\color{redstrings}]  % Start your code-block

    public void setValue(int newValue) throws InterruptedException {
        List<Observer> shallowCopy;
        sem.acquire();
        value = newValue;
        shallowCopy = new ArrayList<>(observers);
        sem.release();

        for (Observer o : shallowCopy) {
            o.notify(this,value);
        }
    }
\end{lstlisting}

%As an example imagine if that the programmer defined code takes the attempts to acquire the same lock that the framework currently holds.  Because the framework holds the locks 

\section{\acl{TL} Characteristics}
This section presents how the \ac{TL} concurrency models relates to the selected characteristics presented in \bsref{chap:char}.

\subsection{Implicit or Explicit Concurrency}
As described in \bsref{subsec:threads} and \bsref{sub:sec:shared_memory}, the \ac{TL} concurrency model specify concurrency and synchronization by starting threads and assessing shared memory. Correctness is assured by locking access critical regions. All of these operations are explicitly stated by the programmer. As such we say that the \ac{TL} concurrency model explicitly states concurrency. The placement is visualized in \bsref{fig:char_implicit_explicit}.

\begin{figure}[htbp]
\centering
 \includegraphics[width=0.9\textwidth]{\rootpath/worksheets/threads_and_locks/figures/tl_char_implicit_explicit} 
 \caption{\ac{TL} on the Implicit - Explicit concurrency spectrum}
\label{fig:char_implicit_explicit}
\end{figure}

\subsection{Fault Restrictive or Expressive Model}
\ac{TL} forces very little upon the programmer. The programmer is left alone to ensure correct execution using locks as well as deciding on a fitting lock granularity. The programmer is given a lot of freedom in expressing concurrency at the cost of having to ensure correct execution herself. Based on these observations we say that \ac{TL} is a expressive concurrency model as shown in \bsref{fig:char_fault_expressive}.

\begin{figure}[htbp]
\centering
 \includegraphics[width=0.9\textwidth]{\rootpath/worksheets/threads_and_locks/figures/tl_char_fault_expressive} 
 \caption{\ac{TL} on the Fault Restrictive - Expressive spectrum}
\label{fig:char_fault_expressive}
\end{figure}

\subsection{Pessimistic or Optimistic Model}
As described in \bsref{sec:locks_me}, the \ac{TL} concurrency model uses locks to ensure mutual exclusion in order to eliminate race conditions. That is, the \ac{TL} concurrency model uses locks to eliminate errors. \ac{TL} assumes that errors will occur and attempts to prevent them. Furthermore, \ac{TL} provides no options for recovery in case of an errors occurs. Based on these observations we say that \ac{TL} is a pessimistic model as shown in \bsref{fig:char_pes_opti}.

\begin{figure}[htbp]
\centering
 \includegraphics[width=0.9\textwidth]{\rootpath/worksheets/threads_and_locks/figures/tl_char_pessimistic_optemistic} 
 \caption{\ac{TL} on the Pessimistic - Optimistic spectrum}
\label{fig:char_pes_opti}
\end{figure}

\subsection{Readability}
As mentioned in \bsref{sec:readability} the evaluation of readability will be based upon that of simplicity and orthogonality as well as a number of other considerations. Simplicity and orthogonality is described in the following sections followed by a final evaluation of readability.
\subsubsection{Simplicity}\label{subsec:tl_simplicity_read}
As described in \bsref{subsec:tl_lock_hard} and \bsref{subsec:tl_composability} the \ac{TL} concurrency model has a number of issue which contributes negatively to its simplicity. When taking multiple locks, taking them in incorrect order might result in a deadlock. Locking is however, as describe in \bsref{subsec:tl_lock_hard}, a global property and the correct order might not be directly discernible by the reader, from within the local context. As such it is no simple task for the reader to reason about whether the order of locking is correct.

\bsref{subsec:tl_composability} describes how how the \ac{TL} model is not composable. Implementing the observer pattern in a thread safe manner was shown to entail a number of issues. The potential for concurrency related programmer errors in similar implementations are high and identifying these errors can potentially be even harder.

In \cite{lee2006problem} the author describes development of the Ptolemy project\cite{lee1999overview}, which involved a significant portion of concurrency. During the development phase of this project, the software engineering approaches of design reviews, code reviews, nightly builds, regression tests, and automated code coverage metrics where employed in order to produce a bug free program \cite[p. 8]{lee2006problem}. Event though the team achieved 100\% code coverage and all code was reviewed by experts, the system still deadlocked after having been in production for four years. Indicating that employing good software engineering principles, might not be enough to catch all concurrency related errors, even among experts.

Based on the issued mentioned here, as well as the sheer number of concurrency related issue that can arise when using the \ac{TL} concurrency model, we say that the \ac{TL} model has low simplicity. Its placements on the readability simplicity spectrum is depicted in \bsref{fig:char_read_simplicity}.

\begin{figure}[htbp]
\centering
 \includegraphics[width=0.9\textwidth]{\rootpath/worksheets/threads_and_locks/figures/tl_char_read_simplicity} 
 \caption{\ac{TL} on the Low - High simplicity spectrum}
\label{fig:char_read_simplicity}
\end{figure}

\kasper[inline]{based heavily on discussion of why TL is hard in dicussion section}
\subsubsection{Orthogonality}
\subsubsection{Readability evaluation}

\subsection{Writability}
\subsubsection{Simplicity}
 The programmer is largely left alone to insure that a concurrent implementation executes correctly and there for also left alone to tackle the issues that the \ac{TL} model has. 
\subsubsection{Orthogonality}
\subsubsection{Level of abstraction}
\subsubsection{Expressivity}
\subsubsection{Writability evaluation}
\worksheetend
