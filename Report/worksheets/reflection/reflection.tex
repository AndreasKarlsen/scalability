\makeatletter \@ifundefined{rootpath}{\input{../../setup/preamble.tex}}\makeatother
\worksheetstart{Reflection of Process}{1}{April 24, 2013}{Andreas}{../../}
This chapter presents a reflection on a number of decisions made throughout the report.
\lone[inline]{Reflection on choice of concurrency models}
\lone[inline]{Evaluation of implementation effort}
\lone[inline]{Reflection on choice of distance measure}
\lone[inline]{Readability and Wriability as requirements. Tightly coupled to implementation}
%\lone[inline]{Count clock cycles since STM can perform extra work, where TL waits (wastes no cycles)}
%\lone[inline]{We wanted to compare the models, but there is variations of the models, which we did not know when we started}
%\lone[inline]{We using models to be more general about the concepts. But we use a specific implementations. There is a dilemma between not judging on the same implementations in concepts and performance, but also wanting to cover more than a specific implementation}
\lone[i]{Reflect on the choice of using only Actor model and not CSP}
\toby[i]{Måske også reflekterer over vores map-reduce valg af i forhold til implementationen - men synes ikke der er så meget at reflektere over}
\kasper[inline]{Boxing af integers i Java implementation. int[] halverede tid sammenlignet med generic ArrayList Integer}

\section{Comparing Models}
Choosing to compare concurrency models instead of specific implementations of the models has presented a number of issues. Implementations of the selected concurrency models vary in how they choose to implement the model. Some actor implementations do not support all semantic properties and \ac{STM} implementations vary greatly in their implementation strategies. At times it proved difficult to distinguish what should be included in the descriptions and evaluations. 

The performance test is based on a single implementation of each concurrency model. We have attempted to select competitive implementations of the models that give a board view of their capabilities but in the end the performance test only compares the selected implementations. We do however believe that such a test can give a indication of what to expect from the models.
\kasper[inline]{More!}

\section{Choice of Concurrency Models}


\section{Choice of Characteristics}
For investigation the characteristics of the selected concurrency model a number of characteristics was selected and discussed in \bsref{chap:char}. Implicit or Explicit Concurrency, Fault Restrictive or Expressive Model and Pessimistic or Optimistic Model where selected as they represent a set of standard characteristics which applies to concurrency models.

Besides these characteristics a number of characteristics based on existing literature for evaluating programming languages where employed. The main focus being readability and writability. Traditionally these two characteristics are supported by a number of other characteristics of which only the subset applicable to evaluating concurrency models where employed. Data types and syntax design where discarded as they do not fit the purpose of the evaluation. The concurrency models do not encompass specific data types and the evaluation focuses on the models instead of a particular syntax.

Besides the selected characteristics it could have been of interest to examine the maintainability of the selected concurrency models. Software maintenance is important as systems can live for many years and therefore must be adapted to the changing needs and corrected. Furthermore, the cost of software maintenance can constitute a significant portion of a software solutions total cost\cite[p. 17]{sebestaProLang}.

Evaluating the characteristics of concurrency models instead of specific implementations did present some difficulty. To support the evaluation a implementation of a concurrent problems such as the dining philosopher problem\cite[p. 673]{hoare1978communicating} or the santa claus problem\cite{trono1994new} could have been created. The evaluation of characteristics could then be based off the implementation, referring to it to clarify decisions. As the goal of this rapport was to evaluate the model and not a concrete implementation care should be taken not to focus on the details of specific concurrency model implementations employed.

\section{Performance Test}\label{sec:reflec_perf_test}
\lone[inline]{k-means is good for parallel, but sucks for concurrency. Realization point: When we realized, all implementations only need to sync when the mappers delivers to the reducers}
The choice to employ the $k$-means clustering algorithm for testing performance proved to be somewhat problematic. Firstly the work done by the algorithm is considered parallel according to \bsref{def:concurrency}. Parallelism builds upon concurrency but the goals are somewhat different. Evaluating the execution time of an inherently concurrent implementation can however be difficult, as the system does not have an overall task for which time can be measured.

Designing test cases that emphasised the performance of the concurrency models proved to be problematic. The exploratory test described in \bsref{sec:performance_test_design} provided some good insights which where used for developing the remaining test cases. I did however also reveal a very small difference between the performance of the selected concurrency models. As the Map-Reduce design of the implementations allowed for very limited use of synchronization, the concurrency model employed had limited impact on the overall performance. Therefore it was decided to create a additional line of testing where the concurrency models where put under a higher level of stress. The resulting test cases are described in \bsref{sec:performance_sync_intensive_desc}. The exploratory test provided the insights needed for designing these test cases.

The original idea was to have two tests: one scaling the number of vectors and one scaling the number of mappers. Each of these tests where to be run for 10 iterations. As the Map-Reduce design of the implementations allowed for very limited use of synchronization, the concurrency model employed had limited impact on the overall performance. As a result, the performance of the implementations were very similar.\toby{Evt. indsæt reference til bilag med resultaterne} To overcome this issue the tests where moved from employing a large dataset over a small number of iterations to employing a smaller dataset and running for a high number of iterations. Reducing the size of dataset limits the time spent on clustering and increasing the number of iterations results in the code segments related to the concurrency models being executed more often. As a result the concurrent models had a larger impact on the results providing a clearer picture of their performance.

Employing an inherently concurrent problem such as the dining philosopher problem\cite[p. 673]{hoare1978communicating} or the santa claus problem\cite{trono1994new} could be an alternative to the $k$-means clustering algorithm. Throughput, such as the number of times philosophers gets to eat within a given time period, could have been measured as an alternative to execution time.

Counting the CPU time spent in addition to real time spent could provide e a more accurate measure of the effort spent by each concurrency model. Especially considering that \ac{STM} may have to abort and retry transactions leading to additional additional CPU time spent.

\section{Common Clustering Code}\toby{Måske anden titel? Implementation technique? anden?}
The choice of using the same code base for the clustering calculations across all concurrency model implementations was done to provide a common ground for the implementations. Utilizing the same clustering code for each implementation ensures that the clustering takes an equal amount of time for all implementations. As such any variations in execution time can be contributed to the concurrency models and not the clustering code. 

%Using the same code may pose the threat of making it difficult or impossible to spilt the $k$-means clustering algorithm up differently for each of the models. One split of the algorithm may fit better for a given model than the others. 

Due to the common cluster code being developed mountainously with the \ac{TL} implementation, it poses the threat of over-fitting the cluster code for the \ac{TL} concurrency model. That is, forcing the mindset of programming with \ac{TL} upon the other concurrency models through the cluster code. The code was however developed to fit the employed Map-Reduce strategy and not the actual \ac{TL} implementation. As such we are not of the impressions that we have over fitted the common code to any of the implementations, as we were able to model both the actor model and \ac{STM} implementation without any restrictions posed from \ac{TL}. However as the cluster code is written in Java, we were forced to use the data structures from Java, such as the Java List, in the actor model and \ac{STM} implementations.

Additionally, during the project we changed the distance measure used for the $k$-means clustering algorithm because we wanted to have a less computational demanding distance measure. As a result of our common clustering code we were able to implement this distance measure a single time and have it take effect across all implementations quickly.


\worksheetend