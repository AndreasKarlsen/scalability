\makeatletter \@ifundefined{rootpath}{\input{../../setup/preamble.tex}}\makeatother
\worksheetstart{Reflection}{1}{April 24, 2013}{Andreas}{../../}
This chapter presents a reflection of decisions made throughout the report. Common for the decisions is that they arguably had an impact of the conclusion. We will discuss the potential impact and reason over the impact the making different decisions.
%\lone[inline]{Reflection on choice of concurrency models}
%\lone[inline]{Evaluation of implementation effort}
%\lone[inline]{Reflection on choice of distance measure}
%\lone[inline]{Readability and Wriability as requirements. Tightly coupled to implementation}
%\lone[inline]{Count clock cycles since STM can perform extra work, where TL waits (wastes no cycles)}
%\lone[inline]{We wanted to compare the models, but there is variations of the models, which we did not know when we started}
%\lone[inline]{We using models to be more general about the concepts. But we use a specific implementations. There is a dilemma between not judging on the same implementations in concepts and performance, but also wanting to cover more than a specific implementation}
%\lone[i]{Reflect on the choice of using only Actor model and not CSP}
%\toby[i]{Måske også reflekterer over vores map-reduce valg af i forhold til implementationen - men synes ikke der er så meget at reflektere over}
%\kasper[inline]{Boxing af integers i Java implementation. int[] halverede tid sammenlignet med generic ArrayList Integer}

\section{Comparing Models}
Choosing to compare concurrency models instead of specific implementations of the models has presented a number of issues. Implementations of the selected concurrency models vary in how they choose to implement the model. Some actor implementations do not support all semantic properties and the strategy vary greatly in the \ac{STM} implementations. At times it proved difficult to distinguish what should be included in the descriptions and evaluations. 

The performance test is based on a single implementation of each concurrency model. We have attempted to select competitive implementations of the models that gives a representative view of their capabilities but in the end the performance test only compares the selected implementations. We do however believe that such a test can give a indication of what to expect from the models. But as we will discuss in \bsref{sec:extended_performance_test}, a range of other tests would give a more complete comparison.

\section{Choice of Concurrency Models}
To select the concurrency models to investigate further, a preliminary investigatio was conducted in \bsref{chap:intro}. The investigation covered \ac{TL}, the actor model, \ac{Rx}, \ac{STM} and \ac{CSP} of which \ac{TL}, the actor model and \ac{STM} where selected for further investigation.

When the concurrency models where initially selected our estimation was that the \ac{TL} concurrency model and \ac{STM} where somewhat dissimilar. During the course of the project similarities between the two concurrency model became more clear. From the point of view of the programmer the two models handle concurrency very similarly, both utilise shared memory and require the application of synchronization to critical regions in order to avoid race conditions. How synchronization is achieved is however different.

\ac{CSP} is in many ways similar to the actor model, substituting actors message passing with channel based communication. Based on this the actor model was preferred as it has good support in a number of languages.

Including a concurrency model with a vastly different focus could have been of interest. As an example selecting a concurrency model with a focus on super computers, such as that of the language X10\cite{tardieu2014x10} described briefly in \bsref{chap:intro}, could provide a new perspective for the report.

\section{Choice of Characteristics}
For investigating the characteristics of the selected concurrency model, a number of characteristics was selected and discussed in \bsref{chap:char}. Implicit or Explicit Concurrency, Fault Restrictive or Expressive Model and Pessimistic or Optimistic Model were selected as they affect the use of concurrency models. Although we choose a wide selected, had other characteristics been chosen, it could have effected the result of the comparison of the characteristics.

Beside these characteristics a number of characteristics based on existing literature for evaluating programming languages where employed. The main focus being readability and writability. Traditionally these two characteristics are supported by a number of other characteristics. Only the subset applicable to evaluating concurrency models where employed. Data types and syntax design where discarded as they do not fit the purpose of the evaluation. The concurrency models do not encompass specific data types and the evaluation focus on the models instead of a particular syntax.

Besides the selected characteristics it could have been of interest to examine the maintainability of the selected concurrency models. Software maintenance is important as systems can live for many years and therefore must be adapted to the changing needs and corrected. Furthermore, the cost of software maintenance can constitute a significant portion of a software solutions total cost\cite[p. 17]{sebestaProLang}.

Evaluating the characteristics of concurrency models instead of specific implementations did present some difficulty. To support the evaluation a implementation of a concurrent problems such as the dining philosopher problem\cite[p. 673]{hoare1978communicating} or the santa claus problem\cite{trono1994new} could have been created. The evaluation of characteristics could then be based off the implementation, referring to it to clarify decisions. As the goal of this rapport was to evaluate the model and not a concrete implementation care should be taken not to focus on the details of specific concurrency model implementations employed.

\section{Performance Test}\label{sec:reflec_perf_test}
The choice to employ the $k$-means clustering algorithm for testing performance proved to be problematic. Firstly the work done by the algorithm is considered parallel according to \bsref{def:concurrency}. Parallelism builds upon concurrency but the goals are different. Evaluating the execution time of an inherently concurrent implementation can however be difficult, as the system does not have an overall task for which time can be measured.

Designing test cases that emphasised the performance of the concurrency models proved to be problematic. The exploratory test described in \bsref{sec:performance_test_design} provided some good insights which were used for developing the remaining test cases. It also revealed a very small difference between the performance of the selected concurrency models. As the Map-Reduce design of the implementations allowed for very limited use of synchronization, the concurrency model employed had limited impact on the overall performance. Therefore it was decided to create an additional line of testing where the concurrency models where put under a higher level of stress. The resulting test cases are described in \bsref{sec:performance_sync_intensive_desc}. The exploratory test provided the insights needed for designing these test cases.

The original idea was to have two tests: one scaling the number of vectors and one scaling the number of mappers. Each of these tests where to be run for 10 iterations. As the Map-Reduce design of the implementations allowed for very limited use of synchronization, the concurrency model employed had limited impact on the overall performance. As a result, the performance of the implementations were very similar. To overcome this issue the tests where moved from employing a large dataset over a small number of iterations to employing a smaller dataset and running for a high number of iterations. Reducing the size of dataset limits the time spent on clustering and increasing the number of iterations results in the code segments related to the concurrency models being executed more often. As a result the concurrent models had a larger impact on the results providing a clearer picture of their performance.

Employing an inherently concurrent problem such as the dining philosopher problem\cite[p. 673]{hoare1978communicating} or the santa claus problem\cite{trono1994new} could be an alternative to the $k$-means clustering algorithm. Throughput, such as the number of times philosophers gets to eat within a given time period, could have been measured as an alternative to execution time.

Counting the CPU time spent in addition to real time spent could provide a more accurate measure of the resources used by each concurrency model. Considering that \ac{STM} may have to abort and retry transactions leading to additional CPU time used, where \ac{TL} would simply wait. The impact of this, depends on the environment which the program runs in. If there is only a single task, all resources should be utilised, and waiting for another thread is considered a time waste. However, if there are other programs running, hogging the resources for failing transactions might lead to overall slower result of executing all the programs.

\section{Shared Clustering Implementation}
The choice of using the same code base for the clustering calculations across all concurrency model implementations was done to provide a common ground for the implementations. Utilising the same clustering code for each implementation ensures that the clustering takes an equal amount of time for all implementations. As such any variations in execution time can be contributed to the concurrency models and not the clustering code. 

%Using the same code may pose the threat of making it difficult or impossible to spilt the $k$-means clustering algorithm up differently for each of the models. One split of the algorithm may fit better for a given model than the others. 

Due to the common cluster code being developed simultaneously with the \ac{TL} implementation, it poses the threat of over-fitting the cluster code for the \ac{TL} concurrency model. That is, forcing the mindset of programming with \ac{TL} upon the other concurrency models through the cluster code. The code was however developed to fit the employed Map-Reduce strategy and not the actual \ac{TL} implementation. As such we are not of the impression that we have over fitted the common code to any of the implementations, as we were able to model both the actor model and \ac{STM} implementation without any restrictions posed from \ac{TL}. However as the cluster code is written in Java, we were forced to use the data structures from Java, such as the Java List, in the actor model and \ac{STM} implementations.

Additionally, during the project we changed the distance measure used for the $k$-means clustering algorithm because we wanted to have a less computational demanding distance measure. As a result of our common clustering code we were able to implement this distance measure a single time and have it take effect across all implementations quickly.


\worksheetend