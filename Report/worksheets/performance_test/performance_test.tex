\makeatletter \@ifundefined{rootpath}{\input{../../setup/preamble.tex}}\makeatother
\worksheetstart{Performance Test}{1}{April 24, 2013}{Kasper}{../../}
%
To investigate the performance aspect of the selected concurrency models, a performance test will be conducted.  This chapter describes the definition of the tests in \bsref{sec:test_approach}, followed by an outlining of the key points in the different implementations in \bsref{sec:test_impl}. In the end we present and analysis the results of the test in \bsref{sec:test_result}.
\label{chap:performance}
\section{Definition of Performance Test}
\label{sec:test_approach}
The $k$-means clustering algorithm has been selected as the basis for the comparison as it is computationally demanding and easily partitioned into concurrent tasks\cite[p. 128]{epstein2011towards}. The $k$-means algorithm has been employed by others to measure scalability performance\cite{epstein2011towards}\cite{tardieu2014x10}.

$k$-means aims at clustering $n$ data points, into $k \leq n$ clusters\cite[p. 451]{dataminingconceptsandtechniques}\cite[p. 128]{epstein2011towards}. Each cluster $k$ is initially assigned a random centroid representing the mean of the cluster. For each of the data points, a distance to the centroid of each of the $k$ clusters is calculated. The data point is then assigned to the cluster to which it is closest. After every data point has been assigned, new centroids are calculated, based on the data points assigned to the clusters. The process is then repeated, using the newly calculated cluster centroids as the basis for the distance calculations. The algorithm terminates when the cluster centroids stop changing or when a user defined number of iterations has been completed\cite[p. 128]{epstein2011towards}. The algorithm is exemplified in \bsref{fig:kmeans}. Centroids are represented by triangles and data points by circles.

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.3]{\rootpath/worksheets/performance_test/figures/kmeans}
\caption{On the left data points are assigned to the nearest centroid (cluster) based on the calculated distances. On the right, a new centroid is computed for each cluster.}\label{fig:kmeans}
\end{figure}

The implementations follow a template similar to that of MapReduce\cite{dean2008mapreduce}, known from functional programming, as well as the Google\cite{dean2008mapreduce} and Hadoop\footnote{\url{http://hadoop.apache.org/}} MapReduce frameworks. MapReduce employs a set of mappers to apply a function to each data point. This function outputs a number of results, that are then aggregated to the final result using a reducer. The $k$-means MapReduce process is illustrated in \bsref{fig:kmeans_mapreduce}. A set of mappers is assigned a set of vectors, for which each of the mappers calculates the distance to each of the $k$ cluster centroids and assigns the centroid to the nearest cluster. A reducer will then aggregate the results from the mappers and calculate the new cluster centroids. The process will continue until the stated number of iterations is achieved.

\begin{figure}
\centering
\includegraphics[scale=0.5]{\rootpath/worksheets/performance_test/figures/mapreduce_figure}
\caption{$k$-means MapReduce process}\label{fig:kmeans_mapreduce}
\end{figure}



\subsection{Test Design}
\label{sec:performance_test_design}
To guide the design of the needed test cases, an initial exploratory test run was done. Examining the results of the initial exploratory test provided knowledge which influenced the design of the remaining test cases. A description of the initial test run as well as its results can be viewed in \bsbilagref{app:explo_result}. The test run showed little difference between the selected concurrency models and revealed a need for both stressing the synchronization mechanisms of the selected concurrency models as well as scaling the size of the workload. Therefore it was decided to investigate both how the concurrency models affect high workload tasks and high synchronization tasks. To this end, two test cases were designed. Both test cases are based on a data set of 100 integer data point vectors. 100 data points was selected as it allows the algorithm to perform a non negligible amount of work without over removing the focus from the concurrency models. The first test investigates how the selected concurrency models affect a task with a high degree of concurrent work but a low degree of synchronization. The second investigates how the selected concurrency models affect a task with a low degree of concurrent work and a high degree of synchronization. To mitigate potential outlying results, each test was run five times and the mean execution time was used as the result. The initial test run showed little variance between each execution of the test. As such five executions should be sufficient to even out the results. In the context of performance analysis, only the effort taken to cluster the data is of interest, as opposed to the resulting clustering. Due to this, randomly generated data is sufficient.

\subsection{Test Setup}\label{subsec:hardware}
Listing the specification of the hardware is mainly due to reproducibility and transparency. The tests will be performed on a high-grade consumer PC, specifically a Macbook Pro from 2014, with the following specifications:
\begin{itemize}
	\item \textbf{CPU} Quad Core 2.8 GHz Intel Core i7 \footnote{\url{http://ark.intel.com/products/83503/Intel-Core-i7-4980HQ-Processor-6M-Cache-up-to-4_00-GHz}}
	\item \textbf{Memory} 16 GB 1600 MHz DDR3L
	\item \textbf{\ac{OS}} OSX 10.10.1 (14B25)
	\item \textbf{\ac{JVM}} Java HotSpot(TM) 64-Bit Server VM (build 25.20-b23, mixed mode)
\end{itemize}
%The result of the performance test is only interesting when comparing the different implementations

The most noticeable specification of the test setup is the amount of cores on the \ac{CPU}. There are 8 logical cores, where 4 of them are physical. The amount of cores will enable us to test the scalability of the different implementations. In some research\cite{harris2003language}, the scalability of different implementations varied after surpassing 35 cores. This is discussed further in \bsref{sec:extended_performance_test}.

The test will be done with the Wi-Fi turned off and all applications closed but the test application. This is done to eliminate other programs hogging resources during the test run. The machine will be plugged into a power outlet the entire time to enable maximum clocking of the \ac{CPU}.

\subsection{Work Intensive Tests}
The work intensive tests consist of the two tests: test $A$ and test $B$. The parameters for the tests are shown in \bsref{tab:test_description_work_intensive}. These tests are designed to investigate the impact of the selected concurrency models on workload with a low degree of synchronization and a large amount of concurrent work.

Test $A$ is based on a fixed size data set, while the number of mapper tasks will be scaled up. The goal is to determine how the selected concurrency models scale as more concurrent tasks are added. Five clusters are created, using 1 to 10 mapper tasks. The algorithm will run for 100 iterations, as this will give an execution time which enables the test to finish within the allotted time (over night). The amount of mappers, has been chosen with consideration to the amount of physical and logical cores in hardware used for the test, described further in \bsref{subsec:hardware}. The implementations use one thread to coordinate and drive the calculations, as well as running the reducer. Consequently, making 3 mapper tasks will utilize 4 cores concurrently. The only thing that differs in the implementations is the concurrency mechanisms and as the number of mappers increases, the contention on the synchronization point increases. Therefore, we expect that the performance of the concurrency models will vary as the amount of mappers increases. The variance caused by the concurrency model may however be small, as time used by the concurrency models will be small, compared to the amount spent on the actual clustering. 

Test $B$ is based on a fixed number of concurrent mapper tasks as well as a fixed number of iterations, while the size of the dataset will be scaled up. Scaling up the size of the dataset will increase the workload run concurrently. The goal of the test is to investigate how the selected concurrency models affects a task with a high concurrent workload. The test will employ a dataset of 1,000,000 to 5,000,000 vectors along with seven mapper tasks. The test will run for 100 iterations. We expect to see a decreasing difference in performance as the vector size increases, since an increasing amount of time will be spend on clustering and little time spent on synchronization, due to the fixed low amount of iterations.

\begin{center}
\begin{table}[h]
\centering
\begin{tabular}{c|cccc}
       & \# Vectors        & \# Mappers			 	& \# Clusters & \# Iterations \\ \hline
Test $A$ & 2,000,000            & 1-10        			& 5           & 100      \\
Test $B$ & 1,000,000-5,000,000  & 7          			& 5           & 100
\end{tabular}
\end{table}
\captionof{table}{Test descriptions for work intensive tests} \label{tab:test_description_work_intensive} 
\end{center}

\subsubsection{Distance Measure}
A large part of the computations involved in the $k$-means algorithm, lies in calculating the distance between each of the data points and the cluster centroids. Different ways of calculating this distance exist. Simple, yet efficient, calculations such as Manhattan and Euclidean\cite[p. 41]{amatriain2011data} distances are fast to compute, whereas the more complex similarity measures such as Pearson Correlation Coefficient and Cosine similarity\cite[p. 42]{amatriain2011data}\cite{breese1998empirical} are slower to compute but provide a more accurate result.

For the work intensive tests the \ac{PCC} will be used, as it represents one of the most complex computations. Having a complex distance calculation will increase the workload that will run concurrently. The \ac{PCC} is defined as\cite[p. 4]{breese1998empirical}:
\begin{equation}\label{pearsonverbose}
\frac{cov(a,i)}{\sigma_a \times \sigma_i} = \frac{\sum_j(v_{a,j}-\bar{v}_a)(v_{i,j}-\bar{v}_i)}{\sqrt{{\sum_j}(v_{a,j}-\bar{v}_a)^2 \sum_j(v_{i,j}-\bar{v}_i)^2}}
\end{equation}

where $cov(a,i)$ is the covariance between the two vectors $a$ and $i$ and $\sigma_a \times \sigma_i$ is the product of the vectors standard deviations. The \ac{PCC} measures similarity between two vectors. If the vectors tend to have high values in the same dimension and low values in the same dimension then they are said to be similar. The product of the standard deviations is used to normalize the result brining it into the range $1$ to $-1$. 

\subsection{Synchronization Intensive Tests}
\label{sec:performance_sync_intensive_desc}
The synchronization intensive test will consist of the two tests: test $C$ and test $D$. The parameters for the tests are shown in \bsref{tab:test_description}. These tests are designed to investigate how the selected concurrency models affect a concurrent computation with a high degree of synchronization. 

Test $C$ is based on a fixed size data set, while the number of mapper tasks will be scaled up.  The test will be based on a data set of 10.000 vectors. Five clusters are created, using 1 to 10 mapper tasks. The algorithm will run for 100.000 iterations. As with the previous tests, the amount of mappers have been chosen based on the underlying hardware. We expect a decreasing run time while scaling up to 7 mappers, as that will utilize all 8 cores concurrently. However, the biggest decrease should be seen up to 3 mappers, as the physical cores are known to perform vastly better than hyperthreaded cores\cite{marr2002hyper}. After 7 mappers, we expect the run time to be slower, as coordination between the threads causes overhead in the computation.

Test $D$ is based on a fixed number of concurrent tasks as well as a fixed size dataset, while the number of iterations will be scaled up. Synchronization is applied whenever mappers delegate their work to the reducer. Scaling up the number of iterations will increase the number of times synchronization is executed for each of the selected models. It is our expectation that a greater difference will be revealed, as the number of iterations increase and thereby the amount of synchronization overhead. The test will employ a dataset of 10,000 vectors along with seven mapper tasks. The number of iterations will be scaled from 10,000 to 100,000 in intervals of 10,000. 7 mappers is chosen, since it will enable all 8 logical cores to work concurrently.

The parameters of the two tests are depicted in \bsref{tab:test_description}.

\begin{center}
\begin{table}[h]
\centering
\begin{tabular}{c|cccc}
       & \# Vectors        & \# Mappers			 	& \# Clusters & \# Iterations \\ \hline
Test $C$ & 10,000            & 1-10        			& 5           & 100,000      \\
Test $D$ & 10,000			 & 7          			& 5           & 10,000-100,000
\end{tabular}
\end{table}
\captionof{table}{Test descriptions for synchronization intensive tests} \label{tab:test_description} 
\end{center}

\subsubsection{Distance Measure}
The manhattan distance will be used for the synchronization intensive tests as it represents a simple distance calculation. Having a simple calculation will limit the amount of work spent on computing the clusters. As such the work done by the concurrency models will have a greater impact on the results. The Manhattan distance is defined as\cite[p. 41]{amatriain2011data}:
 
\begin{equation}\label{eq:mandistance}
man(v_a,v_i)=\sum_{j=1}^{n}\lvert v_{a,j}-v_{i,j}\rvert
\end{equation}

where $man(v_a,v_i)$ is the Manhattan distance between the two vectors $v_a$ and $v_i$ and $n$ is the length of these vectors. $v_{a,j}$ and $v_{i,j}$ are the $j$'th item of the vectors $v_a$ and $v_i$ respectively. It calculates the sum of the differences between the two vectors, $v_{a}$ and $v_{i}$ on a given item, $j$.


%

\section{Test Implementations}\label{sec:test_impl}
This section presents a small overview of the implementations used in the performance test. As described in \bsref{sec:intro_scope}, one implementation has been made for each of the selected concurrency models, and all of the chosen languages run on the \ac{JVM}. The \ac{TL} based implementation is done in Java, the \ac{STM} based implementation in Clojure and the actor based implementation in Scala. The calculations needed to cluster vectors is implemented in Java and reused throughout all implementations. This ensures that the computations themselves do not impact the results. Each implementation simply creates its own skeleton for driving the clustering process using its corresponding concurrency model, and employs the common clustering implementation to handle the clustering details. 

In order to provide a fair comparison between the implementations execution time, the amount of work they do must be equivalent. As the implementations share the same code base for cluster calculations, it is only the concurrency mechanism and driving skeleton which impacts the results. To verify this, we have made a static set of vectors, which we can run across all the implementations, and compare the result of the final means. Running over this test data all implementations produce the same clustering. While this is not a formal verification, is does provide a positive indication that the same work is performed across the implementations. The source code is available at the enclosed DVD together with instructions for running it.
%
\subsection{\ac{TL} Implementation}
Java has been chosen for the \ac{TL} based implementation because of its well documented support for the concurrency model and its widespread use. The \ac{TL} implementation maps the reducer and mappers directly to their own thread by providing an implementation of the \bscode{Runnable} interface. A producer consumer setup, controlled by a semaphore, is employed. The mappers cluster the subset of data which they receive and pass the result to the reducer by adding it to a queue. Finally the reducer is signalled via the semaphore. The reducer aggregate the results as they become available from the mappers  and prepares for the next iteration by calculating the new cluster means. The \bscode{run} method of the \bscode{Mapper} class is shown in \bsref{lst:tl_implementation}. On line 3 the mapper uses the common clustering service to cluster the data it received according to the supplied means. The resulting clustering is added to a queue shared with the reducer and all other mappers on line 6. Access to the queue is synchronized by using a lock, preventing other mappers, as well as the reducer from accessing the queue while the mapper adds the result. Finally the mapper signals the reducer using the semaphore on line 8. All non local variables, such as the data and queue, are supplied in the \bscode{Mapper} classes constructor.

\begin{lstlisting}[float,label=lst:tl_implementation,
  caption={\ac{TL} Implementation},
  language=Java,  
  showspaces=false,
  showtabs=false,
  breaklines=true,
  showstringspaces=false,
  breakatwhitespace=true,
  commentstyle=\color{greencomments},
  keywordstyle=\color{bluekeywords},
  stringstyle=\color{redstrings}]  % Start your code-block

    @Override
    public void run() {
        Clustering clustering = ClusteringService.ClusterKMeansMSIncremental(data, means);
        //Hand off data
        lock.lock();
        queue.add(clustering);
        lock.unlock();
        semaphore.release(1);
    }  
\end{lstlisting}

\subsection{\ac{STM} Implementation}
As our preliminary investigation showed in \bsref{sec:prelim_stm}, a number of programming languages and libraries to support \ac{STM} have emerged. To our knowledge, Clojure is the only official language running on the \ac{JVM} that natively support \ac{STM}. The native support enables compiler and runtime optimisations, which potentially is a big performance factor. Due to these reasons, Clojure has been chosen as the language for the \ac{STM} implementation.

\subsubsection{Clojures \acs{STM} Design}
In \bsref{subsec:stm:variations_in_design} we discussed the impact of \ac{STM} design variations. In the following section, we will briefly describe the \ac{STM} design in Clojure.

The isolation level of transactions in Clojure is strong. Clojure employs a special type for references named \bscode{ref}\footnote{\url{http://clojuredocs.org/clojure.core/ref}}. It is only possible to modify this reference  inside transactions. Due to this, Clojure does not have to account for modifications of references happening outside of transactions, and can secure strong isolation without any performance penalty. This is a property made possible due to transactions being built into the language. 

It is the programmers responsibility not to do any irreversible actions, such as \ac{IO}, in a transaction. Clojure supplies a macro, \bscode{!io}, which raises a runtime exception if a marked function is called in a transaction. If an exception is raised inside a transaction, the transaction is aborted and the exception is propagated out of the transaction\footnote{\url{http://clojuredocs.org/clojure.core/dosync}}. As discussed in \bsref{subsec:stm:side_effects}, this does not change the way the programmer should handle exceptions compared to sequential code. 

It is however not possible to wrap a transactional block around an existing implementation, to secure it runs correct concurrently. This is due to the special \bscode{ref} construct in Clojure, that must be used to enable atomicity in a transaction. The composability of \ac{STM} and the other language constructs in Clojure is limited by design. It is only possible to modify the aforementioned special types in a transaction. Other types in Clojure cannot leverage transactions to synchronize, but this is part of the design, as they are immutable. 

It is possible to nest transactions in Clojure, however the inner transaction will first commit when the outer transaction does.

Conditional synchronization is not directly supported in Clojure, as the \bscode{dosync} function does not take any conditional variables and the language does not have a \bscode{retry} construct. As such conditional synchronization must be achieved by other means, such as busy waiting.

Reads in Clojure are invisible\cite{nielsen2010benchmarking}, which means that when a transaction reads, it does not communicate its operations to other transactions. Instead metadata is used to validate each transaction. To manage contention, Clojure uses a strategy called Priority\cite{nielsen2010benchmarking}. This strategy allows older transactions, in respect to alive time, to run in favour of younger transactions. Research\cite{nielsen2010benchmarking} has shown that in most cases this is a competitive strategy compared to other contention strategies. 

\subsubsection{Clojure Implementation}
\begin{lstlisting}[float,label=lst:stm_implementation,
  caption={\ac{STM} Implementation},
  language=clojure,  
  showspaces=false,
  showtabs=false,
  breaklines=true,
  showstringspaces=false,
  breakatwhitespace=true,
  commentstyle=\color{greencomments},
  keywordstyle=\color{bluekeywords},
  stringstyle=\color{redstrings}]  % Start your code-block
  
  (def clusters (ref '()))
    
  (defn mapper [data means clusters]
    (let [cluster (clustering data means)]
      (dosync
        (commute clusters conj cluster)))
    clusters)
\end{lstlisting}

The central point of the implementation, is shown at line 1 in \bsref{lst:stm_implementation}. It shows a reference to at list, named \bscode{clusters}. When calculating the centroids, a vector containing the initial values, is split up into the amount of mappers defined. The mappers work on each of their slice, and add their result to the \bscode{clusters} list. The reducer then reduces the list, by merging the results and calculating the centroids.

Line 3 in \bsref{lst:stm_implementation} shows the definition of the mapper function. Each mapper is executed in a \bscode{future}\footnote{\url{http://clojuredocs.org/clojure.core/future}}, which is a concurrency unit, that yields a non-blocking result. The future can be queried for the result, on which it will block until the result is available. On line 4 is the actual calculation which is bound to a value named \bscode{cluster}. The \bscode{cluster} value is then added into the list of \bscode{clusters} on line 6, by using the \bscode{commute}\footnote{\url{http://clojuredocs.org/clojure.core/commute}} function. \bscode{commute} is a special function in Clojure, that can be used to update references inside transactions, when the update is commutative. That is, the order the list is updated in, does not matter. For non-commutative operations, \bscode{alter}\footnote{\url{http://clojuredocs.org/clojure.core/alter}} can be used, albeit it is not as fast as \bscode{commute}\cite{kumar2013clojure}. All this happens inside the \bscode{dosync} transaction block, that ensures synchronisation in concurrent operations. The reducer must wait until all the mappers have updated the list with their result. This is enforced by dereferencing the futures, that the mappers are executed in, which will force synchronisation, so the reducer receives all the intermediate clustering results.

\subsection{Actor Implementation}
The Scala language has been selected for the actor based implementation, as it has good support for actors in the form of the Akka actor framework. Akka is a well-known and popular actor implementation \cite[p. 4-5]{tasharofi2014efficient} used by companies such as Cisco, Ebay, Amazon and Blizzard.\footnote{\url{http://akka.io/}} Akka has even replaced the Scala Actors as the default actor framework for Scala applications\footnote{Changes in Ver. 2.10.0: \url{http://www.scala-lang.org/download/changelog.html}}.

\subsubsection{Akka Adherence to Semantic Properties}
The Akka framework uses light-weight actors which means that several actors may share a single thread\cite[p. 13]{akkaDoc}, as discussed in \bsref{ssec:abstraction_over_threads}. 
In relation to the semantic properties of the actor model, mentioned in \bsref{ssec:actor_s_properties}, Akka adheres to the property of atomic processing of messages and location transparency. Location transparency is ensured by using actor references, that are created by specifying a name together with an actor type\cite[p. 24]{akkaDoc}. 

However, the properties of fairness and encapsulation are not adhered to, in order to improve performance. Akka employs an at-most-once message delivery semantics which means that a message is either delivered zero or one times, as that has the lowest implementation overhead\cite[p. 27]{akkaDoc}. This violates the fairness property which assumes that all messages are eventually delivered. However, it is possible to enforce stricter reliability in Akka, but at the cost of local-only deployment\cite[p. 29]{akkaDoc}. Akka does also not adhere to the encapsulation property, as it implements message passing by sending message contents by reference which enables the possibility of shared mutable state, as discussed in \bsref{sssec:safe_msg_passing}. However this only applies to local actors, when messages are sent to remote actors the message contents are copied instead. Additionally, these are the default serialization settings and Akka enables the programmer to specify different serialization techniques\cite[p. 219]{akkaDoc}.

As Akka does not adhere to all the semantic properties of the actor model, it introduce risks that the conceptual actor model avoids by design, e.g. deadlocks. To alleviate these issues Akka has some actor best practices, that the programmer should adhere to. E.g. do not pass mutable objects between actors\cite[p. 12]{akkaDoc}. It is a trade-off between programmer concerns and implementation performance.

%MERE:
%Akka facilitates fault toleracne, persistence and automatic load balancing.
	% se side 1-2 i Akka scala documentation
	% og se efficient testing artikeln s. 12 (under actors)

%Quote fra efficient testing artikeln:
%Note that exploiting the benefits of the actor model for programming does not require a language that enforces strict actor semantics; it is sufficient to have a library providing asynchronous messaging between concurrent objects, and to adhere to coding conventions for avoiding shared state.
\subsubsection{Akka Implementation}
A central \bscode{Master} actor is responsible for driving the clustering process. It handles distribution of data to the mappers and running the clustering process for the required number of iterations. \bsref{lst:actor_implementation} shows the \bscode{Mapper} and \bscode{Reducer} actors of the implementation. On line 3-7 the behaviour method scope is defined for the \bscode{Mapper} actor. There is only a single behaviour method defined for this actor, which is for a \bscode{MapWork} message on line 4. Such a message contains the data that should be clustered, the cluster centroids and a reference to the \bscode{Reducer} actor, sent from the \bscode{Master} actor. On line 5 the \bscode{Mapper} clusters its data segment using the common clustering service, followed on line 6 by sending the result as a \bscode{MapperResult} message to the \bscode{Reducer}.

On line 10-24 the \bscode{Reducer} actor is defined. First on line 11-12 some isolated state variables are defined, followed by the behaviour method scope on line 14-24.
The \bscode{Reducer} actor receives the result from \bscode{Mappers} on line 15, where the \bscode{Reducer} merges the received intermediate result with the previously received results. On line 19-22, if the current message is the last message then the \bscode{Reducer} calculates the means for the new clusters and sends the \bscode{ReducerResult} message to the master actor.

\begin{lstlisting}[float,label=lst:actor_implementation,
  caption={Actor Implementation},
  language=Scala,  
  showspaces=false,
  showtabs=false,
  breaklines=true,
  showstringspaces=false,
  breakatwhitespace=true,
  commentstyle=\color{greencomments},
  keywordstyle=\color{bluekeywords},
  stringstyle=\color{redstrings}]  % Start your code-block

  class Mapper extends Actor {

    def receive = {
      case MapWork(data: List[Vector], means: List[Vector], reducer: ActorRef) =>
        val clustering: Clustering = ClusteringService.ClusterKMeansMSIncremental(data, means)
        reducer ! MapperResult(clustering)
    }
  }

  class Reducer(nrActors: Int, nrClusters: Int) extends Actor {
    var consumedMessages: Int = 0
    var clustering: Clustering = new Clustering(nrClusters)

    def receive = {
      case MapperResult(c: Clustering) =>
        clustering.mergeWith(c);
        consumedMessages += 1

        if (nrActors == consumedMessages) {
            clustering.calcMeansUsingMeanSum();
            context.parent ! ReducerResult(clustering)
        }
    }
  }  
\end{lstlisting}

\section{Test Results}\label{sec:test_result}
This section will present and analyse the results of the performance tests. The tests were conducted as described in \bsref{sec:test_approach}. The purpose of the analysis is to verify our expectations, and establish knowledge that can be used to compare the concurrency models with each other.

\subsection{Work Intensive Tests}
The goal of test $A$ was to determine how the concurrency models are affected with an increased amount of concurrent work. This was done by having 2,000,000 vectors as data, scaling 1 - 10 mappers, running 100 iterations, and a computational demanding distance measure. The results in \bsref{table:test_results_a} show little to no difference between the models when scaling the mappers. This is visualised in \bsref{fig:tr_scale_mappers_a}. As expected, a greater difference is seen as the amount of mappers are scaled. From 8 to 10 mappers there is some difference between the models where the actor model is slightly slower than the other models. This difference is visualised in \bsref{fig:value_norm_testa}. Conclusively, the models scale almost identically to the amount of concurrency. The average shown in \bsref{fig:value_norm_testa} also confirm this.
%
\begin{center}
\begin{table}[h]
\centering
\begin{tabular}{l|cll|cll}
             & \multicolumn{3}{c|}{Milliseconds} & \multicolumn{3}{c}{Normalized} \\
Mappers & TL     & STM     & Actor     & TL      & STM      & Actor     \\ \hline
1   &   569016	  &      570666      &    569449	&  1,00   & 1,00 &    1,00    \\
2   &   292167      &      292164      &    292131  &  1,00   & 1,00 &    1,00    \\
3   &   245074      &      245541      &    244911  &  1,00   & 1,00 &    1,00    \\
4   &   193194      &      192827      &    193198  &  1,00   & 1,00 &    1,00    \\
5   &   173611      &      173347      &    172562  &  1,01   & 1,00 &    1,00    \\
6   &   152898      &      152783      &    152597  &  1,00   & 1,00 &    1,00    \\
7   &   137845      &      137265      &    137859  &  1,00   & 1,00 &    1,00    \\
8   &   124775      &      124575      &    126722  &  1,00   & 1,00 &    1,02    \\
9   &   122880      &      122736      &    128297  &  1,00   & 1,00 &    1,05    \\
10 &   124332      &      123822      &    129408  &  1,00   & 1,00 &    1,05    \\
\end{tabular}
\captionof{table}{Test $A$ results. Scaling the amount of mappers}\label{table:test_results_a}
\end{table}
\end{center}
%
\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=1.2]
	\begin{axis}[
		legend entries={\acs{TL},\acs{STM}, Actor},
		legend pos=north east,
		grid = both,
		xlabel=Mappers,
		ylabel=Milliseconds,
		xmin=0,
		ymin=0, 
		label=fig:tr_scale_mappers_a]		
	\addplot[color=red,mark=x] coordinates {
%		(1,21)
        (1, 569016)
        (2, 292167)
        (3, 245074)
        (4, 193194)
        (5, 173611)
        (6, 152898)
        (7, 137845)
        (8, 124775)
        (9, 122880)
        (10,124332)
	};
	\addplot[color=blue,mark=x] coordinates {
%		(1,33)
        (1, 570666)
        (2, 292164)
        (3, 245541)
        (4, 192827)
        (5, 173347)
        (6, 152783)
        (7, 137265)
        (8, 124575)
        (9, 122736)
        (10,123822)
	};
	\addplot[color=green,mark=x] coordinates {
%		(1,			27)
        (1, 569449)
        (2, 292131)
        (3, 244911)
        (4, 193198)
        (5, 172562)
        (6, 152597)
        (7, 137859)
        (8, 126722)
        (9, 128297)
        (10,129408)
	};
	\end{axis}
\end{tikzpicture}
%\captionsetup{justification=centerlast, margin=10ex, labelfont=bf, textfont=it, format=plain, labelformat=default, labelsep=endash, font=small}
\caption{Test $A$. Visual representation of results for scaling number of mappers}\label{fig:tr_scale_mappers_a}
\end{figure}
%
\begin{figure}[h]
\centering
%0 - aramente   1 - Às vezes   2 - Quase sempre   4 - Sempre
\pgfplotstableread{
  %TL    	%STM  	%Actor
0 1.00  1.00 1.00
1 1.00  1.00 1.00
2 1.00	1.00 1.02
3 1.00	1.00 1.05
4 1.00	1.00 1.01

}\dataset
\begin{tikzpicture}
\begin{axis}[ybar,
%        width=12cm,
		width=\textwidth,
        height=8cm,
        ymin=0.8,
        ymax=1.2,        
        ylabel={Value-normalized execution time},
        xlabel={Mappers},
        xtick=data,
        xticklabels = {
            \strut 2,
            \strut 4,
            \strut 8,
            \strut 10,
            \strut average
            %Category 5,
            %Category 6
        },
        %xticklabel style={yshift=-10ex},
        major x tick style = {opacity=0},
        minor x tick num = 1,
        minor tick length=2ex,
        every node near coord/.append style={
                anchor=west,
                rotate=90
        },
        legend entries={\ac{TL} ,\ac{STM} ,Actor },
        legend columns=3,
        legend style={draw=none,nodes={inner sep=3pt}},
        ]
\addplot[draw=black,fill=red, nodes near coords] table[x index=0,y index=1] \dataset; %TL
\addplot[draw=black,fill=blue, nodes near coords] table[x index=0,y index=2] \dataset; %STM
\addplot[draw=black,fill=green, nodes near coords] table[x index=0,y index=3] \dataset; %Actor
\end{axis}
\end{tikzpicture}
%\captionsetup{justification=centerlast, margin=10ex, labelfont=bf, textfont=it, format=plain, labelformat=default, labelsep=endash, font=small}
\caption{Test $A$. Value-normalized results for scaling number of mappers}\label{fig:value_norm_testa}
\end{figure}
%Test B
%Goal: Determine how the concurrency models scale with added workload \\
%Expectations: Decrease in difference as the vector size increases.\\
%
The goal of test $B$ was to determine how the concurrency models scaled with added workload. This was done by scaling the data amount with 1,000,000 to 5,000,000 vectors, having 7 mappers, 100 iterations, and a computational demanding distance measure. The results in table \bsref{table:test_results_b} show that performance of all three concurrency models is almost equal when scaling the vectors. This is visualised in \bsref{fig:test_results_b_iterations}. Our initial expectation was to see a decreasing difference between the selected models as the workload was scaled up. The initial difference was however so small that no such decrease was detected.  \bsref{fig:value_norm_testb} visualises the differences of the value-normalized execution time. The average confirms that overall there is little to no difference between the concurrency models when scaling the vectors. The actor model does however pull slightly ahead towards the end. The fact that Akka passes messages by reference instead of by value is a contributing factor to this results, as a deep copy of large data structures is avoided. 
%
\FloatBarrier
\begin{center}
\begin{table}[h]
\centering
\begin{tabular}{l|cll|cll}
             & \multicolumn{3}{c|}{Milliseconds} & \multicolumn{3}{c}{Normalized} \\
Vectors & TL     & STM     & Actor     & TL      & STM      & Actor     \\ \hline
1.000.000                   &     68572		 &      68806        &    68541	   &	1,00   & 1,00 &    1,00    \\
2.000.000                   &     137677      &      138041      &    137249  &  1,00   & 1,01 &    1,00    \\
3.000.000                   &     206000      &      206559      &    205472  &  1,00   & 1,01 &    1,00    \\
4.000.000                   &     274622      &      275882      &    273920  &  1,00   & 1,01 &    1,00    \\
5.000.000                   &     344108      &      344939      &    343521  &  1,00   & 1,00 &    1,00    \\
6.000.000                   &     456160      &      467615      &    453272  &  1,01   & 1,03 &    1,00    \\
7.000.000                   &     658594      &      658302      &    625962  &  1,05   & 1,05 &    1,00    \\
\end{tabular}
\captionof{table}{Test $B$ results. Scaling the amount of vectors}\label{table:test_results_b}
\end{table}
\end{center}

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=1.2]
	\begin{axis}[
		legend entries={\acs{TL},\acs{STM}, Actor},
		legend pos=north west,
		grid = both,
		xlabel=Vectors,
		ylabel=Milliseconds,
		xmin=0,
		ymin=0, 
		label=fig:tr_scale_iterations]		
	\addplot[color=red,mark=x] coordinates {
%		(1,21)
		(100000,68572)
		(200000,137677)
		(300000,206000)
		(400000,274622)
		(500000,344108)
		(600000,456160)
		(700000,658594)
	};
	\addplot[color=blue,mark=x] coordinates {
%		(1,33)
		(100000,68806)
		(200000,138041)
		(300000,206559)
		(400000,275882)
		(500000,344939)
		(600000,467615)
		(700000,658302)
	};
	\addplot[color=green,mark=x] coordinates {
%		(1,			27)
		(100000,68541)
		(200000,137249)
		(300000,205472)
		(400000,273920)
		(500000,343521)
		(600000,453272)
		(700000,625962)
	};
	\end{axis}
\end{tikzpicture}
%\captionsetup{justification=centerlast, margin=10ex, labelfont=bf, textfont=it, format=plain, labelformat=default, labelsep=endash, font=small}
\caption{Test $B$. Visual representation of results for scaling number of vectors}\label{fig:test_results_b_iterations}
\end{figure}
%
\begin{figure}[h]
\centering
%0 - aramente   1 - Às vezes   2 - Quase sempre   4 - Sempre
\pgfplotstableread{
  %TL    	%STM  	%Actor
0 1.00	1.01	1.00
1 1.00	1.01	1.00
2 1.01	1.03	1.00
3 1.05	1.05	1.00
4 1.01	1.02	1.00
}\dataset
\begin{tikzpicture}
\begin{axis}[ybar,
%        width=12cm,
		width=\textwidth,
        height=8cm,
        ymin=0.8,
        ymax=1.2,        
        ylabel={Value-normalized execution time},
        xlabel={Vectors},
        xtick=data,
        xticklabels = {
            \strut 2\,000\,000,
            \strut 4\,000\,000,
            \strut 6\,000\,000,
            \strut 7\,000\,000,
            \strut average
            %Category 5,
            %Category 6
        },
        %xticklabel style={yshift=-10ex},
        major x tick style = {opacity=0},
        minor x tick num = 1,
        minor tick length=2ex,
        every node near coord/.append style={
                anchor=west,
                rotate=90
        },
        legend entries={\ac{TL} ,\ac{STM} ,Actor },
        legend columns=3,
        legend style={draw=none,nodes={inner sep=3pt}},
        ]
\addplot[draw=black,fill=red, nodes near coords] table[x index=0,y index=1] \dataset; %TL
\addplot[draw=black,fill=blue, nodes near coords] table[x index=0,y index=2] \dataset; %STM
\addplot[draw=black,fill=green, nodes near coords] table[x index=0,y index=3] \dataset; %Actor
\end{axis}
\end{tikzpicture}
%\captionsetup{justification=centerlast, margin=10ex, labelfont=bf, textfont=it, format=plain, labelformat=default, labelsep=endash, font=small}
\caption{Test $B$. Value-normalized results for scaling number of vectors}\label{fig:value_norm_testb}
\end{figure}
%
Overall we can conclude, that the performance of the concurrency models under high workload does not differ much, neither when scaling the workload or concurrency. The difference is negligible, unless performance is vital. If that is the case, \ac{TL} and \ac{STM} are marginally preferable in highly concurrent tasks.%, and the actor model in tasks with large data amounts.
%
\FloatBarrier
\subsection{Synchronization Intensive Tests}
%TEST C
%Goal: Test the impact of scaling the concurrent work in a synchronization intensive test\\
%Expectation: Decreased execution time up to 7 mappers, biggest decreases after 3 mappers.\\
%
The goal of test $C$ was to see the impact of scaling the concurrent work in a synchronization intensive scenario. This was done by scaling from 1 to 10 mappers, a data size of 10,000 vectors, 100,000 iterations, and a computational simple distance measure. The results displayed in \bsref{table:test_results_concurrent_tasksc} show that when the amount of synchronization increases, the differences between the concurrency models become more apparent. \bsref{fig:test_results_concurrent_tasks_c} visualises this tendency, and shows that the overall execution time follows the same tendency. As expected, it shows a huge decrease between 1 and 3 mappers, and a continued decrease until 7 mappers. \ac{TL} is fastest in all the cases. In the fastest case with 7 mappers, \ac{STM} and the actor model are respectively 4\% and 7\% slower than the \ac{TL} concurrency model. In the task with most synchronization with 10 mappers, \ac{STM} and the actor model was 12\% and 10\% slower respectively. The difference in performance between the models increases as the amount of synchronization is scaled up. \bsref{fig:value_norm_testc} visualises this, and shows that in the average case, \ac{TL} is the fastest, and \ac{STM} and the actor model is 6\% and 5\% slower relatively.
%
\begin{center}
\begin{table}[h]
\centering
\begin{tabular}{l|cll|cll}
             & \multicolumn{3}{c|}{Milliseconds} & \multicolumn{3}{c}{Normalized} \\
Mapper Tasks & TL     & STM     & Actor     & TL      & STM      & Actor     \\ \hline
1                   &     498792      &      510154      &    513734  &	 1.00   & 1.02 &    1.03    \\
2                   &     263507      &      272869      &    268115  &  1.00   & 1.04 &    1.02    \\
3                   &     186577      &      194757      &    191510  &  1.00   & 1.04 &    1.03    \\
4                   &     159645      &      173548      &    161304  &  1.00   & 1.09 &    1.01    \\
5                   &     171512      &      176791      &    173379  &  1.00   & 1.03 &    1.01    \\
6                   &     154108      &      163118      &    161729  &  1.00   & 1.06 &    1.05    \\
7                   &     152569      &      158746      &    163118  &  1.00   & 1.04 &    1.07    \\
8                   &     154261      &      168380      &    162539  &  1.00   & 1.09 &    1.05    \\
9                   &     171477      &      188020      &    188653  &  1.00   & 1.10 &    1.10    \\
10                 &     163490      &      182868      &    179347  &	 1.00   & 1.12 &    1.10    \\
\end{tabular}
\captionof{table}{Test $C$ results. Scaling number of mapper tasks}\label{table:test_results_concurrent_tasksc}
\end{table}
\end{center}
%
\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=1.2]
	\begin{axis}[
		legend entries={\acs{TL},\acs{STM}, Actor},
		legend pos=north east,
		grid = both,
		xlabel=Mapper tasks,
		ylabel=Milliseconds,
		xmin=0,
		ymin=0, 
		label=fig:tr_scale_mappers]		
	\addplot[color=red,mark=x] coordinates {
		(1,498792)
		(2,263507)
		(3,186577)
		(4,159645)
		(5,171512)
		(6,154108)
		(7,152569)
		(8,154261)
		(9,171477)
		(10,163490)
	};
	\addplot[color=blue,mark=x] coordinates {
		(1,510154)
		(2,272869)
		(3,194757)
		(4,173548)
		(5,176791)
		(6,163118)
		(7,158746)
		(8,168380)
		(9,188020)
		(10,182868)
	};
	\addplot[color=green,mark=x] coordinates {
		(1,513734)
		(2,268115)
		(3,191510)
		(4,161304)
		(5,173379)
		(6,161729)
		(7,163118)
		(8,162539)
		(9,188653)
		(10,179347)
	};
	\end{axis}
\end{tikzpicture}
%\captionsetup{justification=centerlast, margin=10ex, labelfont=bf, textfont=it, format=plain, labelformat=default, labelsep=endash, font=small}
\caption{Test $C$ results. Visual representation of results for scaling number of mapper tasks}\label{fig:test_results_concurrent_tasks_c}
\end{figure}

\begin{figure}[h]
\centering
%0 - aramente   1 - Às vezes   2 - Quase sempre   4 - Sempre
\pgfplotstableread{
  %TL    	%STM  	%Actor
0	1		1.02		1.03
1	1		1.04		1.02
2	1		1.04		1.03
3	1		1.09		1.01
4	1		1.06		1.05
}\dataset
\begin{tikzpicture}
\begin{axis}[ybar,
		height=8cm,
		width=\textwidth,
        ymin=0.8,
        ymax=1.2,        
        ylabel={Value-normalized execution time},
        xlabel={Mappers},
        xtick=data,
        xticklabels = {
            \strut 1,
            \strut 2,
            \strut 3,
            \strut 4,
            \strut average
            %Category 5,
            %Category 6
        },
        %xticklabel style={yshift=-10ex},
        major x tick style = {opacity=0},
        minor x tick num = 1,
        minor tick length=2ex,
        every node near coord/.append style={
                anchor=west,
                rotate=90
        },
        legend entries={\ac{TL} ,\ac{STM} ,Actor },
        legend columns=3,
        legend style={draw=none,nodes={inner sep=3pt}},
        ]
\addplot[draw=black,fill=red, nodes near coords] table[x index=0,y index=1] \dataset; %ano de 2013-2014
\addplot[draw=black,fill=blue, nodes near coords] table[x index=0,y index=2] \dataset; %ano de 2012-2013
\addplot[draw=black,fill=green, nodes near coords] table[x index=0,y index=3] \dataset; %ano de 2011-2012
\end{axis}
\end{tikzpicture}
%\captionsetup{justification=centerlast, margin=10ex, labelfont=bf, textfont=it, format=plain, labelformat=default, labelsep=endash, font=small}
\caption{Test $C$ results. Value-normalized results for scaling number of mapper tasks}\label{fig:value_norm_testc}
\end{figure}
%TEST D
%Goal: Scaling the amount of synchronization, enables us to see how more synchronization impacts the execution time\\
%Expectation: More difference between the different concurrency models as we scale iterations\\
%
The goal of test $D$ was to see the effect of the synchronization mechanisms when scaling the amount of synchronization. This was done by scaling the iterations from 10,000 to 100,000, having a data size of 10,000 vectors, 7 mappers, and a computational simple distance measure. \bsref{table:test_results_iterations_d} shows the result, where the differences between the models is varying, this is visualised in \bsref{fig:test_results_iterations_d}. In all cases, \ac{TL} is the fastest, and the actor model is  5\% in the best case, and 8\% slower in the worst. Compared to \ac{TL}, \ac{STM} is 1\% slower in the best case, and 10\% slower in the worst case. Overall, \ac{TL} is the fastest, followed by \ac{STM} and then the actor model. This is visualised in \bsref{fig:value_norm_testd}.% The expectation of an increased difference between the concurrency models did happen, however the different models performs differently under different amount of iterations. This is surprising, but could be explained as statistical uncertainty\andreas{Andre gode forklaringer?}.
%
\FloatBarrier
\begin{center}
\begin{table}[h]
\centering
\begin{tabular}{l|cll|cll}
             & \multicolumn{3}{c|}{Milliseconds} & \multicolumn{3}{c}{Normalized} \\
Iterations & TL     & STM     & Actor     & TL      & STM      & Actor     \\ \hline
%1  			&		21			&      33			&		27      & 1.00  & 1.57  &   1.29    \\
10000		&       15368		&      16909		&		16308   & 1.00  & 1.10  &   1.06    \\
20000		&       30643		&      33148		&		32427   & 1.00  & 1.08  &   1.06    \\
30000		&		45743		&      48981		&		48966   & 1.00  & 1.07  &   1.07    \\
40000		&		61097		&      63800		&		64514   & 1.00  & 1.04  &   1.06    \\
50000		&		76983		&      78855    	&		81393   & 1.00  & 1.02  &   1.06    \\
60000		&       93241		&      93930		&		98206   & 1.00  & 1.01  &   1.05    \\
70000		&       108792		&      110237   	&		114076  & 1.00  & 1.01  &   1.05    \\
80000		&		122150		&      126545	    &		131388  & 1.00  & 1.04  &   1.08    \\
90000		&		137349		&      141548   	&		147275  & 1.00  & 1.03  &   1.07    \\
100000	    &		152569		&      158746	    &		163118  & 1.00  & 1.04  &   1.07    \\
\end{tabular}
\captionof{table}{Test $D$ results. Scaling number of iterations}\label{table:test_results_iterations_d}
\end{table}
\end{center}
%
\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=1.2]
	\begin{axis}[
		legend entries={\acs{TL},\acs{STM}, Actor},
		legend pos=north west,
		grid = both,
		xlabel=Iterations,
		ylabel=Milliseconds,
		xmin=0,
		ymin=0, 
		label=fig:tr_scale_iterations]		
	\addplot[color=red,mark=x] coordinates {
%		(1,21)
		(10000,15368)
		(20000,30643)
		(30000,45743)
		(40000,61097)
		(50000,76983)
		(60000,93241)
		(70000,108792)
		(80000,122150)
		(90000,137349)
		(100000,152569)
	};
	\addplot[color=blue,mark=x] coordinates {
%		(1,33)
		(10000,16909)
		(20000,33148)
		(30000,48981)
		(40000,63800)
		(50000,78855)
		(60000,93930)
		(70000,110237)
		(80000,126545)
		(90000,141548)
		(100000,158746)
	};
	\addplot[color=green,mark=x] coordinates {
%		(1,			27)
		(10000,	16308)
		(20000,	32427)
		(30000,	48966)
		(40000,	64514)
		(50000,	81393)
		(60000,	98206)
		(70000,	114076)
		(80000,	131388)
		(90000,	147275)
		(100000,	163118)
	};
	\end{axis}
\end{tikzpicture}
%\captionsetup{justification=centerlast, margin=10ex, labelfont=bf, textfont=it, format=plain, labelformat=default, labelsep=endash, font=small}
\caption{Test $D$. Visual representation of results for scaling number of iterations}\label{fig:test_results_iterations_d}
\end{figure}
%
\begin{figure}[h]
\centering
%0 - aramente   1 - Às vezes   2 - Quase sempre   4 - Sempre
\pgfplotstableread{
  %TL    	%STM  	%Actor
0 1			1.08			1.06
1 1			1.02			1.06
2 1			1.04			1.08
3 1			1.04			1.07
4 1			1.04			1.06
}\dataset
\begin{tikzpicture}
\begin{axis}[ybar,
		width=\textwidth,
        height=8cm,
        ymin=0.8,
        ymax=1.2,        
        ylabel={Value-normalized execution time},
        xlabel={Iterations},
        xtick=data,
        xticklabels = {
            \strut 20\,000,
            \strut 50\,000,
            \strut 80\,000,
            \strut 100\,000,
            \strut average
            %Category 5,
            %Category 6
        },
        %xticklabel style={yshift=-10ex},
        major x tick style = {opacity=0},
        minor x tick num = 1,
        minor tick length=2ex,
        every node near coord/.append style={
                anchor=west,
                rotate=90
        },
        legend entries={\ac{TL} ,\ac{STM} ,Actor },
        legend columns=3,
        legend style={draw=none,nodes={inner sep=3pt}},
        ]
\addplot[draw=black,fill=red, nodes near coords] table[x index=0,y index=1] \dataset; %TL
\addplot[draw=black,fill=blue, nodes near coords] table[x index=0,y index=2] \dataset; %STM
\addplot[draw=black,fill=green, nodes near coords] table[x index=0,y index=3] \dataset; %Actor
\end{axis}
\end{tikzpicture}
%\captionsetup{justification=centerlast, margin=10ex, labelfont=bf, textfont=it, format=plain, labelformat=default, labelsep=endash, font=small}
\caption{Test $D$. Value-normalized results for scaling number of iterations}\label{fig:value_norm_testd}
\end{figure}
%
Overall we can conclude, that the performance of the concurrency models under synchronization intensive workload does vary. When scaling concurrency, the \ac{TL} performs the best, followed by the actor model and \ac{STM}. However, even though \ac{TL} is the fastest, the difference in the worst case is only 12\%, and in average 5\% to 6\%. When scaling the iterations, \ac{TL} was on average marginally faster, although the difference is negligible unless performance is vital.
%


\worksheetend