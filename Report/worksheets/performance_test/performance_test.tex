\makeatletter \@ifundefined{rootpath}{\input{../../setup/preamble.tex}}\makeatother
\worksheetstart{Performance test}{1}{April 24, 2013}{Kasper}{../../}

In order to investigate the performance of the selected models, a performance test will be conducted. This chapter describes the test process along with the implementations and results. 
\label{chap:performance}
\section{Definition of Performance Test Approach}
The $k$-means clustering algorithm has been selected as the basis for the comparison. $k$-means is a good basis for performance analysis of concurrency models\toby{Det er vel ikke helt sandt? Men mere parallel} as it is computationally demanding and easily partitioned into concurrent tasks\cite[p. 128]{epstein2011towards}. The $k$-means algorithm has been employed by others to measure scalability performance\cite{epstein2011towards}\cite{tardieu2014x10} The N-body problem used for comparison in \cite{totoo2012haskell} was considered as an alternative.

$k$-means aims at clustering $n$ data points, into $k \leq n$ clusters\cite[p. 451]{dataminingconceptsandtechniques}\cite[p. 128]{epstein2011towards}. Each cluster $k$ is initially assigned a random centroid representing the mean of the cluster. For each of the data points, a distance to the centroid of each of the $k$ clusters is calculated. The data point is then assigned to the cluster to which it is closest. After every data point has been assigned, new centroids are calculated, based on the data points assigned to the clusters. The process is then repeated, using the newly calculated clusters centroids as the basis for the distance calculations. The algorithm terminates when the cluster centroids stop changing or when a set number of iterations has been completed\cite[p. 128]{epstein2011towards}. The algorithm is exemplified in \bsref{fig:kmeans}. Centroids are represented by triangles and data points by circles.

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.3]{\rootpath/worksheets/performance_test/figures/kmeans}
\caption{On the left data points are assigned to the nearest centroid (cluster) based on the calculated distances. On the right, a new centroid is computed for each cluster.}\label{fig:kmeans}
\end{figure}

Two tests, each with their own purpose, will be conducted. Both tests will be based on a data set of vectors, where each vector consists of 100 integer data points. In the context of performance analysis, only the effort taken to cluster the data is of interest, as opposed to the resulting clustering. As a result, randomly generated data is sufficient.

The first test will be based on a fixed size data set, while the number of concurrent tasks will be scaled up. The goal is to determine how the selected concurrency models, scale as more concurrent tasks are added. The test will be based on a data set of 1,000,000 vectors. Five clusters will be created, using a maximum of eight concurrent tasks. The algorithm will run for 10 iterations.

The second second test will be based on a fixed number of concurrent tasks, while the size of the dataset will be scaled up. The goal of this test is to determine how the selected concurrency model, scale as the size of the dataset increases. The test will be conducted using four concurrent tasks, while scaling the data set from 100,000 vectors to 5,000,000 vectors. As with the previous test, five clusters will be created and the algorithm will run for 10 iterations.

The parameters of the two tests are depicted in \bsref{tab:test_description}.

\begin{center}
\begin{table}[h]
\begin{tabular}{c|cccc}
       & \# Vectors        & \# Concurrent tasks 	& \# Clusters & \# Iterations \\ \hline
Test 1 & 1,000,000         & 1-8        			& 5           & 10            \\
Test 2 & 100,000-5,000,000 & 4          			& 5           & 10           
\end{tabular}
\end{table}
\captionof{table}{Description of tests} \label{tab:test_description} 
\end{center}

\kasper[inline]{Hardware specs}
\begin{center}
\begin{table}[h]
\begin{tabular}{c|cccc}
         & CPU & Memory & Harddrive & Operating system \\ \hline
Hardware &     &        &           &                 
\end{tabular}
\end{table}
\captionof{table}{Hardware specifications} \label{tab:hardware_specs} 
\end{center}

The implementations will follow a template similar to that of Map-Reduce\cite{dean2008mapreduce}, known from functional programming, as well as the Google and Hadoop MapReduce frameworks. MapReduce employs a set of mappers to apply a function to each data point. This function outputs a number of results, that are then aggregated to the final result using a reducer. This is analogous to the \bscode{map} and \bscode{reduce} functions, known from functional programming. For the $k$-means performance test, a set of mappers will be assigned a set of vectors, for which it will calculate the distance to each of the $k$ cluster centroids and assign it to the nearest cluster. A reducer will then aggregate the results from the mappers and calculate the new cluster centroids. The process will continue until the stated 10 iterations are achieved.

\subsection{Distance Measure}
A large part of the computations involved in the $k$-means algorithm, lies in calculating the distance between each of the data points and the cluster centroids. Different ways of calculating this distance exists. Simple, yet efficient, calculations such as Manhattan and Euclidean\cite[p. 41]{amatriain2011data} distances are fast to compute. Where the more complex similarity measures such as Pearson Correlation Coefficient and Cosine similarity\cite[p. 42]{amatriain2011data}\cite{breese1998empirical} are slower to compute but provide a more accurate result.

\kasper[inline]{Remember to draw in the choice of distance measure in reflection}
The \ac{PCC} will be used for the performance test, as it represents one of the most complex computations. Having a complex distance calculation will increase the workload which will be run concurrently, increasing the focus on the concurrency aspect of the performance test. We are aware that choosing a complex calculation, pushes the results towards favoring concurrent execution over sequential execution. The \ac{PCC} is 
defined as:

\begin{equation}\label{pearsonverbose}
\frac{cov(a,i)}{\sigma_a \times \sigma_i} = \frac{\sum_j(v_{a,j}-\bar{v}_a)(v_{i,j}-\bar{v}_i)}{\sqrt{{\sum_j}(v_{a,j}-\bar{v}_a)^2 \sum_j(v_{i,j}-\bar{v}_i)^2}}
\end{equation}

where $cov(a,i)$ is the covariance between the two vectors $a$ and $i$ and $\sigma_a \times \sigma_i$ is the product of the vectors standard deviations. 
\section{Test Implementations}
This section presents a small overview of the implementations used in the performance test. As described in \bsref{sec:intro_scope}, one implementation has been made for each of the selected concurrency models, and all of the chosen languages run on the \ac{JVM}. The \ac{TL} based implementation is done in Java, the \ac{STM} based implementation in Clojure and the actor based implementation in Scala. The calculations needed to cluster vectors is implemented in Java and reused throughout all implementations. This ensures that the computations themselves do not impact the results. Each implementation simply creates its own skeleton for driving the clustering process using its corresponding concurrency model, and employs the common clustering implementation to handle the clustering details. 

\subsection{\ac{TL} Implementation}
Java has been chosen for the \ac{TL} based implementation because of its well documented support for the concurrency model and widespread use. The \ac{TL} implementation maps the reducer and mappers directly to their own thread by providing an implementation of the \bscode{Runnable} interface.\toby{Måske lige hårdt nok at gå igang, enten hav hovedideen i hvordan vi vil implementere k-means i forhold til en map-reduce tankegang før hver implementation, eller forklar det i TL afsnittet her.} A producer consumer setup, controlled by a semaphore, is employed. The mappers cluster the subset of data which they receive and pass the result to the reducer by adding it to a queue. Finally the reducer is signalled via the semaphore. The reducer aggregate the results as they become available from the mappers  and prepares for the next iteration by calculating the new cluster means. The \bscode{run} method of the \bscode{Mapper} class is shown in \bsref{lst:tl_implementation}. On line 3 the mapper uses the common clustering service to cluster the data it received according to the supplied means. The resulting clustering is added to a queue shared with the reducer and all other mappers on line 6. Access to the queue is synchronized by using a lock, preventing other mappers, as well as the reducer from accessing the queue while the mapper adds the result. Finally the reducer is signalling the reducer using the semaphore on line 8. All non local variables, such as the data and queue, are supplied in the \bscode{Mapper} classes constructor. 

\begin{lstlisting}[float,label=lst:tl_implementation,
  caption={\ac{TL} Implementation},
  language=Java,  
  showspaces=false,
  showtabs=false,
  breaklines=true,
  showstringspaces=false,
  breakatwhitespace=true,
  commentstyle=\color{greencomments},
  keywordstyle=\color{bluekeywords},
  stringstyle=\color{redstrings}]  % Start your code-block

    @Override
    public void run() {
        Clustering clustering = ClusteringService.ClusterKMeansMSIncremental(data, means);
        //Hand off data
        lock.lock();
        queue.add(clustering);
        lock.unlock();
        semaphore.release(1);
    }  
\end{lstlisting}

\subsection{\ac{STM} Implementation}

\subsection{Actor Implementation}
The Scala language has been selected for the actor based implementation, as it has good support for actors in the form of the Akka actor framework. Akka is a well-known and popular actor implementation\toby{kan cite "efficient testing artikeln" om at det er en populær actor implementation hvis det er} used by companies such as Cisco, Ebay, Amazon and Blizzard\footnote{\url{http://akka.io/}}. Akka has even replaced the Scala Actors as the default actor framework for Scala applications\footnote{Changes in Ver. 2.10.0: \url{http://www.scala-lang.org/download/changelog.html}}.

\kasper[inline]{Actor properties}
%better performance, automatic load-balancing, improves resilience and fault-tolerance, and introduces opaque actor references for better encapsulation.
%Akka does not guarantee message delivery and therefore does not provide the fairness property of the actor semantics.
%Akka infrastructure guarantees sender-receiver constraint (so messages are recevied in the 
%Coordination pattern: Akka does not implement local synchronization constraints
%Does not guarenntee safe message and use references in messsage to get better performance
%Også i forhold til at være et library generelt (altså hvis man kan kalde metoder fordi en actor er et object igennem Scala's almindeige kode)
	%programmer have to adhere to coding conventions for avoiding shared state use in
%light-weight threads
	%which decreases the ease of use and some of the gains that the conceptual actor model facilitates
	
%both Scala actor and Akka libraries do not enforce the programmers to adhere to the basics of the actor model 

%%PROPERTIES
%atomic processing of messages: overholdes
%fairness: breakes
%encapsulation: breakes pga. message contents kopirers med reference (så man kan få adgang til en anden actros state gennem en message ref)
	% Men man kan selv specificere en anden serilization strategi, hvor man kan vælge det skal deep-copies hvis det er
	% Yderligere Men: det er sikret at andet kode ikke kan kalde metoder direkte på aktoerer, men kun gennem messages
%location transpareceny: overholdes, man skal ikke for hver enkelt actor specificere den physical location
	%http://doc.akka.io/docs/akka/snapshot/general/remoting.html


A central \bscode{Master} actor is responsible for driving the clustering process. It handles distribution of data to the mappers and running the clustering process for the required number of iterations. \bsref{lst:actor_implementation} shows the \bscode{Mapper} and \bscode{Reducer} actors of the implementation. On line 3-7 the behaviour method scope is defined for the \bscode{Mapper} actor. There is only a single behaviour method defined for this actor, which is for a \bscode{MapWork} message on line 4. Such a message contains the data that should be clustered, the cluster centroids and a reference to the \bscode{Reducer} actor, sent from the \bscode{Master} actor. On line 5 the \bscode{Mapper} clusters its data segment using the common clustering service, followed on line 6 by sending the result as a \bscode{MapperResult} message to the \bscode{Reducer}.

On line 10-24 the \bscode{Reducer} actor is defined. First on line 11-12 some isolated state variables are defined, followed by the behaviour method scope on line 14-24.
The \bscode{Reducer} actor receives the result from \bscode{Mappers} on line 15, where the \bscode{Reducer} merges the received intermediate result with the previously received results. On line 19-22, if the current message is the last message then the \bscode{Reducer} calculates the means for the new clusters and sends the \bscode{ReducerResult} message to the master actor.

\begin{lstlisting}[float,label=lst:actor_implementation,
  caption={Actor Implementation},
  language=Scala,  
  showspaces=false,
  showtabs=false,
  breaklines=true,
  showstringspaces=false,
  breakatwhitespace=true,
  commentstyle=\color{greencomments},
  keywordstyle=\color{bluekeywords},
  stringstyle=\color{redstrings}]  % Start your code-block

  class Mapper extends Actor {

    def receive = {
      case MapWork(data: List[Vector], means: List[Vector], reducer: ActorRef) =>
        val clustering: Clustering = ClusteringService.ClusterKMeansMSIncremental(data, means)
        reducer ! MapperResult(clustering)
    }
  }

  class Reducer(nrActors: Int, nrClusters: Int) extends Actor {
    var consumedMessages: Int = 0
    var clustering: Clustering = new Clustering(nrClusters)

    def receive = {
      case MapperResult(c: Clustering) =>
        clustering.mergeWith(c);
        consumedMessages += 1

        if (nrActors == consumedMessages) {
            clustering.calcMeansUsingMeanSum();
            context.parent ! ReducerResult(clustering)
        }
    }
  }  
\end{lstlisting}


\section{Test Results}
\begin{tikzpicture}
	\begin{axis}[
		legend entries={\ac{TL},Actor,\ac{STM}},
		legend pos=north west,
		grid = both,
		xlabel=\#vectors,
		ylabel=Miliseconds,
		xmin=0,
		ymin=0, 
		title=Test results for scaling \#vectors,
		label=fig:tr_scale_vectors]		
	\addplot[color=red,mark=x] coordinates {
		(100000,2)
		(200000,4)
		(500000,10)
		(1000000,22)
		(2000000,47)
		(3000000,63)
		(4000000,83)
		(5000000,113)
	};
	\addplot[color=blue,mark=x] coordinates {
		(100000,10)
		(200000,20)
		(500000,30)
		(1000000,40)
		(2000000,50)
		(3000000,60)
		(4000000,70)
		(5000000,80)
	};
	\addplot[color=green,mark=x] coordinates {
		(100000,20)
		(200000,30)
		(500000,40)
		(1000000,50)
		(2000000,60)
		(3000000,70)
		(4000000,80)
		(5000000,90)
	};
	\end{axis}
\end{tikzpicture}


\worksheetend