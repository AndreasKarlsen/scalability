\makeatletter \@ifundefined{rootpath}{\input{../../setup/preamble.tex}}\makeatother
\worksheetstart{Performance Test}{1}{April 24, 2013}{Kasper}{../../}
%
In order to investigate the performance of the selected models, a performance test will be conducted. This chapter describes the test process along with the implementations and results. 
\label{chap:performance}
\section{Definition of Performance Test Approach}
The $k$-means clustering algorithm has been selected as the basis for the comparison. $k$-means is computationally demanding and easily partitioned into concurrent tasks\cite[p. 128]{epstein2011towards}. The $k$-means algorithm has been employed by others to measure scalability performance\cite{epstein2011towards}\cite{tardieu2014x10} The N-body problem used for comparison in \cite{totoo2012haskell} was considered as an alternative.

$k$-means aims at clustering $n$ data points, into $k \leq n$ clusters\cite[p. 451]{dataminingconceptsandtechniques}\cite[p. 128]{epstein2011towards}. Each cluster $k$ is initially assigned a random centroid representing the mean of the cluster. For each of the data points, a distance to the centroid of each of the $k$ clusters is calculated. The data point is then assigned to the cluster to which it is closest. After every data point has been assigned, new centroids are calculated, based on the data points assigned to the clusters. The process is then repeated, using the newly calculated clusters centroids as the basis for the distance calculations. The algorithm terminates when the cluster centroids stop changing or when a set number of iterations has been completed\cite[p. 128]{epstein2011towards}. The algorithm is exemplified in \bsref{fig:kmeans}. Centroids are represented by triangles and data points by circles.

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.3]{\rootpath/worksheets/performance_test/figures/kmeans}
\caption{On the left data points are assigned to the nearest centroid (cluster) based on the calculated distances. On the right, a new centroid is computed for each cluster.}\label{fig:kmeans}
\end{figure}

The implementations will follow a template similar to that of Map-Reduce\cite{dean2008mapreduce}, known from functional programming, as well as the Google and Hadoop MapReduce frameworks. MapReduce employs a set of mappers to apply a function to each data point. This function outputs a number of results, that are then aggregated to the final result using a reducer. This is analogous to the \bscode{map} and \bscode{reduce} functions, known from functional programming. Our $k$-means MapReduce process is illustrated in \bsref{fig:kmeans_mapreduce}. A set of mappers will be assigned a set of vectors, for which it will calculate the distance to each of the $k$ cluster centroids and assign it to the nearest cluster. A reducer will then aggregate the results from the mappers and calculate the new cluster centroids. The process will continue until the stated number of iterations are achieved.

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.5]{\rootpath/worksheets/performance_test/figures/mapreduce_figure}
\caption{$k$-means MapReduce process}\label{fig:kmeans_mapreduce}
\end{figure}

Two tests, each with their own purpose, will be conducted. Both tests will be based on a data set of vectors, where each vector consists of 100 integer data points. Each test will be run five times and the used result will be the mean runtime. In the context of performance analysis, only the effort taken to cluster the data is of interest, as opposed to the resulting clustering. Due to this, randomly generated data is sufficient.

The first test will be based on a fixed size data set, while the number of mapper tasks will be scaled up. The goal is to determine how the selected concurrency models, scale as more concurrent tasks are added. The test will be based on a data set of 2,000,000 vectors. Five clusters will be created, using a maximum of 7 mapper tasks. The algorithm will run for 10 iterations.

The second second test will be based on a fixed number of concurrent tasks as well as a fixed size dataset, while the number of iterations will be scaled up.  Synchronization is applied whenever mappers hand over their work to the reducer. Scaling up the number of iterations will scale up the number of times synchronization code for each of the selected models is executed. As such this test seeks to investigate the impact each synchronization mechanism has on the performance of the clustering algorithm. The test will employ a dataset of 2,000,000 vectors along with four mapper tasks. The number of iterations will be scaled from one to 100 in intervals of 10.

The parameters of the two tests are depicted in \bsref{tab:test_description}.

\begin{center}
\begin{table}[h]
\centering
\begin{tabular}{c|cccc}
       & \# Vectors        & \# Mappers			 	& \# Clusters & \# Iterations \\ \hline
Test 1 & 2,000,000         & 1-7        			& 5           & 10            \\
Test 2 & 2,000,000		   & 4          			& 5           & 1-100           
\end{tabular}
\end{table}
\captionof{table}{Description of tests} \label{tab:test_description} 
\end{center}

\subsection{Distance Measure}
A large part of the computations involved in the $k$-means algorithm, lies in calculating the distance between each of the data points and the cluster centroids. Different ways of calculating this distance exists. Simple, yet efficient, calculations such as Manhattan and Euclidean\cite[p. 41]{amatriain2011data} distances are fast to compute. Where the more complex similarity measures such as Pearson Correlation Coefficient and Cosine similarity\cite[p. 42]{amatriain2011data}\cite{breese1998empirical} are slower to compute but provide a more accurate result.

\kasper[inline]{Remember to draw in the choice of distance measure in reflection}
The \ac{PCC} will be used for the performance test, as it represents one of the most complex computations. Having a complex distance calculation will increase the workload which will be run concurrently, increasing the focus on the concurrency aspect of the performance test. We are aware that choosing a complex calculation, pushes the results towards favoring concurrent execution over sequential execution. The \ac{PCC} is 
defined as:

\begin{equation}\label{pearsonverbose}
\frac{cov(a,i)}{\sigma_a \times \sigma_i} = \frac{\sum_j(v_{a,j}-\bar{v}_a)(v_{i,j}-\bar{v}_i)}{\sqrt{{\sum_j}(v_{a,j}-\bar{v}_a)^2 \sum_j(v_{i,j}-\bar{v}_i)^2}}
\end{equation}

where $cov(a,i)$ is the covariance between the two vectors $a$ and $i$ and $\sigma_a \times \sigma_i$ is the product of the vectors standard deviations. 
%
\subsection{Hardware}
As difference in execution time, between the implementations is in focus, the absolute execution time barely matters. Listing the specification of the hardware, is mainly due to reproducibility and transparency.
The tests will be performed on a high-grade consumer PC. Macbook Pro from 2014, with the following specifications:
\begin{itemize}
	\item \textbf{CPU} 2.8 GHz Intel Core i7 \footnote{\url{http://ark.intel.com/products/83503/Intel-Core-i7-4980HQ-Processor-6M-Cache-up-to-4_00-GHz}}
	\item \textbf{Memory} 16 GB 1600 MHz DDR3L
	\item \textbf{\ac{OS}} OSX 10.10.1 (14B25)
	\item \textbf{\ac{JVM}} Java HotSpot(TM) 64-Bit Server VM (build 25.20-b23, mixed mode)
\end{itemize}
%The result of the performance test is only interesting when comparing the different implementations

The most noticeable specification of the test setup, is the amount of cores on the \ac{CPU}. There are 8 logical cores, where 4 of them are physical. The amount of cores will enable us to test the scalability of the different implementations. In some research\ac{harris2003language}, the scalability of different implementations varied after surpassing 35 cores. While it could be interesting to employ our test in such a setting, it is not available to us.

\andreas{Maybe put in the jvm parameters}
\section{Test Implementations}
This section presents a small overview of the implementations used in the performance test. As described in \bsref{sec:intro_scope}, one implementation has been made for each of the selected concurrency models, and all of the chosen languages run on the \ac{JVM}. The \ac{TL} based implementation is done in Java, the \ac{STM} based implementation in Clojure and the actor based implementation in Scala. The calculations needed to cluster vectors is implemented in Java and reused throughout all implementations. This ensures that the computations themselves do not impact the results. Each implementation simply creates its own skeleton for driving the clustering process using its corresponding concurrency model, and employs the common clustering implementation to handle the clustering details. 

In order to ensure a fair comparison between the implementations execution time, the amount of work they do must be equivalent. As the implementations share the same code base for cluster calculations, it is only the concurrency mechanism and driving skeleton that we must ensure does the same. To verify this, we have made a static set of vectors, which we can run across all the implementations, and compare the result of the final means. We can verify, that the result is the same, and while we cannot guarantee that the work done is identical, this is a positive indication that the same work is performed across the implementations.\toby{Se hvad Lone sagde til den originale s√¶tning der var her og om det her er bedre.}
\andreas[inline]{The source code is available at the enclosed DVD together with instructions for running it}

\subsection{\ac{TL} Implementation}
Java has been chosen for the \ac{TL} based implementation because of its well documented support for the concurrency model and widespread use. The \ac{TL} implementation maps the reducer and mappers directly to their own thread by providing an implementation of the \bscode{Runnable} interface. A producer consumer setup, controlled by a semaphore, is employed. The mappers cluster the subset of data which they receive and pass the result to the reducer by adding it to a queue. Finally the reducer is signalled via the semaphore.\andreas{Who is the producer/consumer?} The reducer aggregate the results as they become available from the mappers  and prepares for the next iteration by calculating the new cluster means. The \bscode{run} method of the \bscode{Mapper} class is shown in \bsref{lst:tl_implementation}. On line 3 the mapper uses the common clustering service to cluster the data it received according to the supplied means. The resulting clustering is added to a queue shared with the reducer and all other mappers on line 6. Access to the queue is synchronized by using a lock, preventing other mappers, as well as the reducer from accessing the queue while the mapper adds the result. Finally the mapper signals the reducer using the semaphore on line 8. All non local variables, such as the data and queue, are supplied in the \bscode{Mapper} classes constructor.

\begin{lstlisting}[float,label=lst:tl_implementation,
  caption={\ac{TL} Implementation},
  language=Java,  
  showspaces=false,
  showtabs=false,
  breaklines=true,
  showstringspaces=false,
  breakatwhitespace=true,
  commentstyle=\color{greencomments},
  keywordstyle=\color{bluekeywords},
  stringstyle=\color{redstrings}]  % Start your code-block

    @Override
    public void run() {
        Clustering clustering = ClusteringService.ClusterKMeansMSIncremental(data, means);
        //Hand off data
        lock.lock();
        queue.add(clustering);
        lock.unlock();
        semaphore.release(1);
    }  
\end{lstlisting}

\subsection{\ac{STM} Implementation}
As our preliminary investigation showed in \bsref{sec:prelim_stm}, a number of programming languages and libraries have emerged to support \ac{STM}. To our best knowledge, Clojure is the only official language running on the \ac{JVM} that natively support \ac{STM}. The native support enables compiler and runtime optimisations, which potentially is a big performance factor. Due to these reasons, Clojure have been chosen as the language for the \ac{STM} implementation

\subsubsection{Clojures \acs{STM} Design}
In \bsnameref{sec:stm_discussion} we discussed the impact of \ac{STM} designs variations. In the following section, we will briefly describe the \ac{STM} design in Clojure.

%Level of Isolation
The isolation level of transaction in Clojure is strong. Clojure employs a special type for references named \bscode{ref}\footnote{\url{http://clojuredocs.org/clojure.core/ref}}. It is only possible to modify this reference  inside transactions. Due to this, Clojure does not have to account for modifications of references happening outside of transactions, and can secure strong isolation without any penalty. This is a property made possible due to transaction being built in to the language. There exists other types of references that can be used to share mutable storage. These are Atoms, Agents, and Var, and they can also only be used inside a transaction block.

%Conditional Synchronization
It is not possible to make use of conditional synchronization in Clojure. The \bscode{dosync} function takes a number of expressions which are evaluation in a transaction. Consequently, \andreas{NOT DONE}

%Exceptions
If an exception is raised inside a transaction, the transaction is aborted and the exception is propagated out of the transaction\footnote{\url{http://clojuredocs.org/clojure.core/dosync}}. As discussed in \bsnameref{subsec:stm:side_effects}, this does not change the way the programmer should handle exceptions compared to sequential code. It is however not possible to wrap a transactional block around an existing implementation, to secure it runs correct concurrently. This is due to the aforementioned special constructs in Clojure, that must be used together with a transaction.

%Irreversible Actions


%Contention level
%Eager vs Lazy updating
%Conflict Detection
%Visible vs Invisible Reads
%Composability




\subsubsection{Implementation}
\begin{lstlisting}[float,label=lst:stm_implementation,
  caption={\ac{STM} Implementation},
  language=clojure,  
  showspaces=false,
  showtabs=false,
  breaklines=true,
  showstringspaces=false,
  breakatwhitespace=true,
  commentstyle=\color{greencomments},
  keywordstyle=\color{bluekeywords},
  stringstyle=\color{redstrings}]  % Start your code-block
  
  (def clusters (ref '()))
    
  (defn mapper [data means clusters]
    (let [cluster (clustering data means)]
      (dosync
        (commute clusters conj cluster)))
    clusters)
\end{lstlisting}

The central point of the implementation, is shown at line 1 in \bsref{lst:stm_implementation}. It is the reference named \bscode{clusters} to a list. When calculating the means a vector, containing the initial values, is split up into the amount of mappers defined. The mappers work on each of their slice, and add their result to the list. The reducer is then using the list to pick up the slices, and reduce by merging the slices and calculating the means. Line 3 in\bsref{lst:stm_implementation} shows the definition of the mapper function. Each mapper is executed in a \bscode{future}\footnote{\url{http://clojuredocs.org/clojure.core/future}}, which is a concurrency unit, that yields a non-blocking result. The future can be queried for the result, in which it will block until the result is delivered. The \bscode{cluster} binding on line 4 is the actual calculation, bound to a value. The \bscode{cluster} value is then added into the list of \bscode{clusters}, by using the \bscode{commute}\footnote{\url{http://clojuredocs.org/clojure.core/commute}} function. \bscode{commute} is a special function in Clojure, that can be used to update references inside transactions, when the update is commutativity. That is, the order the list is updated in, does not matter. For non-commutative operations, \bscode{alter}\footnote{\url{http://clojuredocs.org/clojure.core/alter}} can be used, albeit it is not as fast as \bscode{commute}.\andreas{Need cite! Noget med, at alter genl√¶ser ved read, g√∏r commute ikke}. All this happens inside the \bscode{dosync} transaction block, that ensures synchronisation in concurrent operations. The reducer must wait until all the mappers have updated the list with their result. This is enforced by dereferencing the future the mapper is executed in, which will force synchronisation, so the reducer have all the slices of the vector.

\subsection{Actor Implementation}
The Scala language has been selected for the actor based implementation, as it has good support for actors in the form of the Akka actor framework. Akka is a well-known and popular actor implementation\toby{kan cite "efficient testing artikeln" om at det er en popular actor implementation hvis det er} used by companies such as Cisco, Ebay, Amazon and Blizzard.\footnote{\url{http://akka.io/}} Akka has even replaced the Scala Actors as the default actor framework for Scala applications\footnote{Changes in Ver. 2.10.0: \url{http://www.scala-lang.org/download/changelog.html}}.

\subsubsection{Akka Adherence to Semantic Properties}\toby{Evt. hav det efter implementation bliver forklaret hvis det passer bedre}
The Akka framework uses light-weight actors which means that several actors may share a single thread\cite[p. 13]{akkaDoc}, as discussed in \bsref{ssec:abstraction_over_threads}. 
In relation to the semantic properties of the actor model, mentioned in \bsref{ssec:actor_s_properties}, Akka adheres to the property of atomic processing of messages and location transparency. Location transparency is ensured by using actor references, that are created by specifying a name together with an actor type\cite[p. 24]{akkaDoc}. 

However, the properties of fairness and encapsulation are not adhered to, in order to improve performance. Akka employs an at-most-once message delivery semantics which means that a message is either delivered zero or one times, as that has the lowest implementation overhead\cite[p. 27]{akkaDoc}. This violates the fairness property which assumes that all messages are eventually delivered. However, it is possible to enforce stricter reliability in Akka, but on the cost of local-only deployment\cite[p. 29]{akkaDoc}. Akka does also not adhere to the encapsulation property, as it implements message passing by sending message contents by reference which enables the possibility of shared mutable state, as discussed in \bsnameref{sssec:safe_msg_passing}. However this only applies to local actors, when messages are sent to remote actors the message contents are instead copied. Additionally, these are only the default serialization settings and Akka enables the programmer to specify different serialization techniques\cite[p. 219]{akkaDoc}.

As Akka does not adhere to all the semantic properties of the actor model, it enables the possibility of issues such as race conditions, that the conceptual actor model avoids by design. To alleviate these issues Akka have some actor best practices, such as do not pass mutable objects between actors, that the programmer should adhere to\cite[p. 12]{akkaDoc}. It is a trade-off between programmer concerns and implementation performance.

%MERE:
%Akka facilitates fault toleracne, persistence and automatic load balancing.
	% se side 1-2 i Akka scala documentation
	% og se efficient testing artikeln s. 12 (under actors)

%Quote fra efficient testing artikeln:
%Note that exploiting the benefits of the actor model for programming does not require a language that enforces strict actor semantics; it is sufficient to have a library providing asynchronous messaging between concurrent objects, and to adhere to coding conventions for avoiding shared state.
\subsubsection{Akka Implementation}
A central \bscode{Master} actor is responsible for driving the clustering process. It handles distribution of data to the mappers and running the clustering process for the required number of iterations. \bsref{lst:actor_implementation} shows the \bscode{Mapper} and \bscode{Reducer} actors of the implementation. On line 3-7 the behaviour method scope is defined for the \bscode{Mapper} actor. There is only a single behaviour method defined for this actor, which is for a \bscode{MapWork} message on line 4. Such a message contains the data that should be clustered, the cluster centroids and a reference to the \bscode{Reducer} actor, sent from the \bscode{Master} actor. On line 5 the \bscode{Mapper} clusters its data segment using the common clustering service, followed on line 6 by sending the result as a \bscode{MapperResult} message to the \bscode{Reducer}.

On line 10-24 the \bscode{Reducer} actor is defined. First on line 11-12 some isolated state variables are defined, followed by the behaviour method scope on line 14-24.
The \bscode{Reducer} actor receives the result from \bscode{Mappers} on line 15, where the \bscode{Reducer} merges the received intermediate result with the previously received results. On line 19-22, if the current message is the last message then the \bscode{Reducer} calculates the means for the new clusters and sends the \bscode{ReducerResult} message to the master actor.

\begin{lstlisting}[float,label=lst:actor_implementation,
  caption={Actor Implementation},
  language=Scala,  
  showspaces=false,
  showtabs=false,
  breaklines=true,
  showstringspaces=false,
  breakatwhitespace=true,
  commentstyle=\color{greencomments},
  keywordstyle=\color{bluekeywords},
  stringstyle=\color{redstrings}]  % Start your code-block

  class Mapper extends Actor {

    def receive = {
      case MapWork(data: List[Vector], means: List[Vector], reducer: ActorRef) =>
        val clustering: Clustering = ClusteringService.ClusterKMeansMSIncremental(data, means)
        reducer ! MapperResult(clustering)
    }
  }

  class Reducer(nrActors: Int, nrClusters: Int) extends Actor {
    var consumedMessages: Int = 0
    var clustering: Clustering = new Clustering(nrClusters)

    def receive = {
      case MapperResult(c: Clustering) =>
        clustering.mergeWith(c);
        consumedMessages += 1

        if (nrActors == consumedMessages) {
            clustering.calcMeansUsingMeanSum();
            context.parent ! ReducerResult(clustering)
        }
    }
  }  
\end{lstlisting}


\section{Test Results}
\begin{tikzpicture}
	\begin{axis}[
		legend entries={\acs{TL},\acs{STM}, Actor},
		legend pos=north west,
		grid = both,
		xlabel=Mapper tasks,
		ylabel=Miliseconds,
		xmin=0,
		ymin=0, 
		title=Test results for scaling number of mapper tasks,
		label=fig:tr_scale_mappers]		
	\addplot[color=red,mark=x] coordinates {
		(1,57491)
		(2,29730)
		(3,25102)
		(4,19166)
		(5,17490)
		(6,15405)
		(7,13888)
	};
	\addplot[color=blue,mark=x] coordinates {
		(1,57491)
		(2,29730)
		(3,25102)
		(4,19166)
		(5,17490)
		(6,15405)
		(7,13888)
	};
	\addplot[color=green,mark=x] coordinates {
		(1,57491)
		(2,29730)
		(3,25102)
		(4,19166)
		(5,17490)
		(6,15405)
		(7,13888)
	};
	\end{axis}
\end{tikzpicture}

\begin{center}
\begin{table}[h]
\centering
\begin{tabular}{c|ccc}
\# Mapper tasks 	& \ac{TL}   & \ac{STM}   & Actor \\ \hline
1                   &           &     57491       &       \\
2                   &           &     29730       &       \\
3                   &           &     25102       &       \\
4                   &           &     19166       &       \\
5                   &           &     17490       &       \\
6                   &           &     15405       &       \\
7                   &           &     13888       &      
\end{tabular}
\end{table}
\label{table:test_results_concurrent_tasks}
\end{center}

\begin{center}
\begin{table}[h]
\centering
\begin{tabular}{c|ccc}
\# Iterations 	& \ac{TL}   & \ac{STM}   & Actor \\ \hline
1     			&           &            &       \\
10	     		&           &            &       \\
20	     		&           &            &       \\
30	    		&           &            &       \\
40	    		&           &            &       \\
50	    		&           &            &       \\
60	    		&           &            &       \\
70	    		&           &            &       \\
80	    		&           &            &       \\
90	    		&           &            &       \\
100	    		&           &            &      
\end{tabular}
\end{table}
\label{table:test_results_iterations}
\end{center}

\worksheetend